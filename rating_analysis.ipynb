{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Anaconda\\envs\\DeepLearning\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "from torch import nn\n",
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv(train_path, test_path):\n",
    "    train_df = pd.read_csv(train_path)\n",
    "    test_df = pd.read_csv(test_path)\n",
    "\n",
    "    train_reviews = train_df[train_df['Product Class'] != 'Else']['review'].tolist()\n",
    "    train_ratings = train_df[train_df['Product Class'] != 'Else']['rating'].tolist()\n",
    "\n",
    "    test_reviews = test_df[test_df['Product Class'] != 'Else']['review'].tolist()\n",
    "    test_ratings = test_df[test_df['Product Class'] != 'Else']['rating'].tolist()\n",
    "\n",
    "    return train_reviews,train_ratings, test_reviews, test_ratings\n",
    "\n",
    "train_reviews,train_ratings, test_reviews, test_ratings = load_csv('./data/drugsComTrain_raw_addclass.csv', './data/drugsComTest_raw_addclass.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10])\n"
     ]
    }
   ],
   "source": [
    "print(torch.unique(torch.tensor(train_ratings)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Anaconda\\envs\\DeepLearning\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2700: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def tokenize(tokenizer, train_reviews, test_reviews):\n",
    "    train_reviews_token = [tokenizer.encode_plus(\n",
    "    text,\n",
    "    truncation=True,\n",
    "    add_special_tokens=True,\n",
    "    max_length=512,            \n",
    "    pad_to_max_length=True,  \n",
    "    return_attention_mask=True,  \n",
    "    return_tensors='pt',      \n",
    "    ) for text in train_reviews]\n",
    "\n",
    "    test_reviews_token = [tokenizer.encode_plus(\n",
    "    text,\n",
    "    truncation=True,\n",
    "    add_special_tokens=True,\n",
    "    max_length=512,            \n",
    "    pad_to_max_length=True,  \n",
    "    return_attention_mask=True,  \n",
    "    return_tensors='pt',      \n",
    "    ) for text in test_reviews]\n",
    "\n",
    "    return train_reviews_token, test_reviews_token\n",
    "\n",
    "\n",
    "train_reviews_token, test_reviews_token = tokenize(tokenizer, train_reviews, test_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Review_Rating_Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, reviews_token, rating):\n",
    "        self.review = reviews_token\n",
    "        self.rating = rating\n",
    " \n",
    "    def __getitem__(self, idx):\n",
    "        item = {k: v.squeeze(dim=0) for k, v in self.review[idx].items()}\n",
    "        item[\"rating\"] = torch.tensor(self.rating[idx] - 1)\n",
    "        return item\n",
    " \n",
    "    def __len__(self):\n",
    "        return len(self.rating)\n",
    "\n",
    "\n",
    "train_dataset = Review_Rating_Dataset(train_reviews_token, train_ratings)\n",
    "test_dataset = Review_Rating_Dataset(test_reviews_token, test_ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertWithMLP(nn.Module):\n",
    "    def __init__(self, bert, hidden_size=768, mlp_hidden_size1=1024, mlp_hidden_size2 =256, num_classes=1):\n",
    "        super(BertWithMLP, self).__init__()\n",
    "        self.bert = bert\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(hidden_size, mlp_hidden_size1),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(mlp_hidden_size1, mlp_hidden_size2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(mlp_hidden_size2, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        cls = outputs.last_hidden_state[:, 0, :]\n",
    "        \n",
    "        logits = self.mlp(cls).squeeze(-1)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 192\u001b[0m\n\u001b[0;32m    189\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest validation error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mmax\u001b[39m(history[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_error\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    191\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 192\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[10], line 175\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    172\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(model_save_path), exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    174\u001b[0m \u001b[38;5;66;03m# 训练和验证\u001b[39;00m\n\u001b[1;32m--> 175\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_and_evaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    176\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    177\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    178\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    179\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    181\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    182\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    183\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_save_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_save_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    184\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_every\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# 每轮都验证\u001b[39;49;00m\n\u001b[0;32m    185\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    187\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTraining complete!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    188\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest validation accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mmax\u001b[39m(history[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_acc\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[10], line 96\u001b[0m, in \u001b[0;36mtrain_and_evaluate\u001b[1;34m(model, train_loader, val_loader, optimizer, criterion, device, epochs, model_save_path, eval_every)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     95\u001b[0m \u001b[38;5;66;03m# 训练阶段\u001b[39;00m\n\u001b[1;32m---> 96\u001b[0m train_loss, train_acc, train_error \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     97\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     98\u001b[0m history[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_loss\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(train_loss)\n\u001b[0;32m     99\u001b[0m history[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_acc\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(train_acc\u001b[38;5;241m.\u001b[39mitem())\n",
      "Cell \u001b[1;32mIn[10], line 23\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[1;34m(model, dataloader, optimizer, criterion, device)\u001b[0m\n\u001b[0;32m     21\u001b[0m correct_predictions \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(preds \u001b[38;5;241m==\u001b[39m labels)\n\u001b[0;32m     22\u001b[0m total_error \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(torch\u001b[38;5;241m.\u001b[39mabs(labels \u001b[38;5;241m-\u001b[39m outputs))\n\u001b[1;32m---> 23\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# 更新进度条显示\u001b[39;00m\n\u001b[0;32m     26\u001b[0m progress_bar\u001b[38;5;241m.\u001b[39mset_postfix({\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m: loss\u001b[38;5;241m.\u001b[39mitem(),\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124macc\u001b[39m\u001b[38;5;124m'\u001b[39m: torch\u001b[38;5;241m.\u001b[39msum(preds \u001b[38;5;241m==\u001b[39m labels)\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(labels),\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m'\u001b[39m: torch\u001b[38;5;241m.\u001b[39mmean(torch\u001b[38;5;241m.\u001b[39mabs(labels \u001b[38;5;241m-\u001b[39m outputs))\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     30\u001b[0m })\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def train_epoch(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_error = 0.0\n",
    "    \n",
    "    progress_bar = tqdm(dataloader, desc=\"Training\", leave=False)\n",
    "    for batch in progress_bar:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['rating'].to(device)\n",
    "\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        loss = criterion(outputs, labels.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        preds = torch.round(outputs)\n",
    "        correct_predictions += torch.sum(preds == labels)\n",
    "        total_error += torch.sum(torch.abs(labels - outputs))\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # 更新进度条显示\n",
    "        progress_bar.set_postfix({\n",
    "            'loss': loss.item(),\n",
    "            'acc': torch.sum(preds == labels).item()/len(labels),\n",
    "            'error': torch.mean(torch.abs(labels - outputs)).item()\n",
    "        })\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = correct_predictions.double() / len(dataloader.dataset)\n",
    "    error = total_error.item() / len(dataloader.dataset)\n",
    "    return avg_loss, accuracy, error\n",
    "\n",
    "def eval_model(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_error = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        progress_bar = tqdm(dataloader, desc=\"Evaluating\", leave=False)\n",
    "        for batch in progress_bar:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['rating'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            loss = criterion(outputs, labels.float())\n",
    "            \n",
    "            preds = torch.round(outputs)\n",
    "            correct_predictions += torch.sum(preds == labels)\n",
    "            total_error += torch.sum(torch.abs(labels - outputs))\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            progress_bar.set_postfix({\n",
    "                'loss': loss.item(),\n",
    "                'acc': torch.sum(preds == labels).item()/len(labels),\n",
    "                'error': torch.mean(torch.abs(labels - outputs)).item()\n",
    "            })\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = correct_predictions.double() / len(dataloader.dataset)\n",
    "    error = total_error.item() / len(dataloader.dataset)\n",
    "    return avg_loss, accuracy, error\n",
    "\n",
    "# 4. 主训练循环\n",
    "def train_and_evaluate(\n",
    "    model, \n",
    "    train_loader, \n",
    "    val_loader, \n",
    "    optimizer, \n",
    "    criterion, \n",
    "    device, \n",
    "    epochs, \n",
    "    model_save_path,\n",
    "    eval_every=1  # 每多少轮评估一次\n",
    "):\n",
    "    best_val_error = 0.0\n",
    "    best_val_acc = 0.0\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'train_acc': [],\n",
    "        'train_error': [],\n",
    "        'val_loss': [],\n",
    "        'val_acc': [],\n",
    "        'val_error': []\n",
    "    }\n",
    "    \n",
    "    for epoch in range(1, epochs+1):\n",
    "        print(f\"\\nEpoch {epoch}/{epochs}\")\n",
    "        \n",
    "        # 训练阶段\n",
    "        train_loss, train_acc, train_error = train_epoch(\n",
    "            model, train_loader, optimizer, criterion, device)\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc.item())\n",
    "        history['train_error'].append(train_error)\n",
    "        \n",
    "        print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | Train Error: {train_error:.4f}\")\n",
    "        \n",
    "        # 验证阶段\n",
    "        if epoch % eval_every == 0 and val_loader is not None:\n",
    "            val_loss, val_acc, val_error = eval_model(\n",
    "                model, val_loader, criterion, device)\n",
    "            history['val_loss'].append(val_loss)\n",
    "            history['val_acc'].append(val_acc.item())\n",
    "            history['val_error'].append(val_error)\n",
    "            \n",
    "            print(f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f} | Val Error: {val_error:.4f}\")\n",
    "            \n",
    "            # 保存最佳模型\n",
    "            if val_error > best_val_error:\n",
    "                best_val_error = val_error\n",
    "                torch.save(model.state_dict(), model_save_path)\n",
    "                print(f\"New best model saved to {model_save_path} with val_acc: {val_acc:.4f} | val_error: {val_error:.4f}\")\n",
    "\n",
    "                continue\n",
    "            \n",
    "            if val_acc > best_val_acc:\n",
    "                best_val_error = val_acc\n",
    "                torch.save(model.state_dict(), model_save_path)\n",
    "                print(f\"New best model saved to {model_save_path} with val_acc: {val_acc:.4f} | val_error: {val_error:.4f}\")\n",
    "    \n",
    "    return history\n",
    "\n",
    "# 5. 主函数\n",
    "def main():\n",
    "    # 初始化\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    BERT = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "    # for param in BERT.parameters():\n",
    "    #     param.requires_grad = False\n",
    "\n",
    "    # layers_to_unfreeze = ['encoder.layer.11', 'encoder.layer.10']  # 解冻最后两层\n",
    "    # for name, param in model.named_parameters():\n",
    "    #     if any(layer in name for layer in layers_to_unfreeze):\n",
    "    #         param.requires_grad = True\n",
    "    \n",
    "    # 加载模型\n",
    "    model = BertWithMLP(BERT, hidden_size=768, mlp_hidden_size1=256, mlp_hidden_size2=10, num_classes=1)\n",
    "    model.to(device)\n",
    "\n",
    "    # for name, param in model.named_parameters():\n",
    "    #     print(name, param.requires_grad)\n",
    "    \n",
    "    # 训练参数\n",
    "    optimizer = torch.optim.Adam([\n",
    "    {'params': model.bert.encoder.layer[-1].parameters(), 'lr': 5e-5},\n",
    "    {'params': model.bert.encoder.layer[-2].parameters(), 'lr': 5e-5},\n",
    "    {'params': model.bert.encoder.layer[-3].parameters(), 'lr': 5e-5},\n",
    "    {'params': model.bert.encoder.layer[-4].parameters(), 'lr': 5e-5},\n",
    "    {'params': model.bert.encoder.layer[-5].parameters(), 'lr': 5e-5},\n",
    "    {'params': model.bert.encoder.layer[-6].parameters(), 'lr': 5e-5},\n",
    "    {'params': model.bert.encoder.layer[-7].parameters(), 'lr': 5e-5},\n",
    "    {'params': model.bert.encoder.layer[-8].parameters(), 'lr': 5e-5},\n",
    "    {'params': model.bert.encoder.layer[-9].parameters(), 'lr': 5e-5},\n",
    "    {'params': model.bert.encoder.layer[-10].parameters(), 'lr': 5e-5},\n",
    "    {'params': model.bert.encoder.layer[-11].parameters(), 'lr': 5e-5},\n",
    "    {'params': model.bert.encoder.layer[-12].parameters(), 'lr': 5e-5},\n",
    "    {'params': model.mlp.parameters(), 'lr': 1e-4}\n",
    "    ])\n",
    "\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    epochs = 20\n",
    "    model_save_path = \"./best_model.pth\"\n",
    "    \n",
    "    # 创建保存目录\n",
    "    os.makedirs(os.path.dirname(model_save_path), exist_ok=True)\n",
    "    \n",
    "    # 训练和验证\n",
    "    history = train_and_evaluate(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=test_loader,\n",
    "        optimizer=optimizer,\n",
    "        criterion=criterion,\n",
    "        device=device,\n",
    "        epochs=epochs,\n",
    "        model_save_path=model_save_path,\n",
    "        eval_every=1  # 每轮都验证\n",
    "    )\n",
    "    \n",
    "    print(\"\\nTraining complete!\")\n",
    "    print(f\"Best validation accuracy: {max(history['val_acc']):.4f}\")\n",
    "    print(f\"Best validation error: {max(history['val_error']):.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
