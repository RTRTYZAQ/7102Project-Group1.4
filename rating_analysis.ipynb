{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from transformers import BertTokenizer, BertModel, get_linear_schedule_with_warmup\n",
    "from torch.optim import AdamW\n",
    "import torch\n",
    "from torch import nn\n",
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv(train_path, test_path):\n",
    "    train_df = pd.read_csv(train_path)\n",
    "    test_df = pd.read_csv(test_path)\n",
    "\n",
    "    train_reviews = train_df[train_df['Product Class'] != 'Else']['review'].tolist()\n",
    "    train_ratings = train_df[train_df['Product Class'] != 'Else']['rating'].tolist()\n",
    "\n",
    "    test_reviews = test_df[test_df['Product Class'] != 'Else']['review'].tolist()\n",
    "    test_ratings = test_df[test_df['Product Class'] != 'Else']['rating'].tolist()\n",
    "\n",
    "    return train_reviews,train_ratings, test_reviews, test_ratings\n",
    "\n",
    "train_reviews,train_ratings, test_reviews, test_ratings = load_csv('./data/drugsComTrain_raw_addclass.csv', './data/drugsComTest_raw_addclass.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.unique(torch.tensor(train_ratings)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(tokenizer, train_reviews, test_reviews):\n",
    "    train_reviews_token = [tokenizer.encode_plus(\n",
    "    text,\n",
    "    truncation=True,\n",
    "    add_special_tokens=True,\n",
    "    max_length=512,            \n",
    "    pad_to_max_length=True,  \n",
    "    return_attention_mask=True,  \n",
    "    return_tensors='pt',      \n",
    "    ) for text in train_reviews]\n",
    "\n",
    "    test_reviews_token = [tokenizer.encode_plus(\n",
    "    text,\n",
    "    truncation=True,\n",
    "    add_special_tokens=True,\n",
    "    max_length=512,            \n",
    "    pad_to_max_length=True,  \n",
    "    return_attention_mask=True,  \n",
    "    return_tensors='pt',      \n",
    "    ) for text in test_reviews]\n",
    "\n",
    "    return train_reviews_token, test_reviews_token\n",
    "\n",
    "\n",
    "train_reviews_token, test_reviews_token = tokenize(tokenizer, train_reviews, test_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Review_Rating_Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, reviews_token, rating):\n",
    "        self.review = reviews_token\n",
    "        self.rating = rating\n",
    " \n",
    "    def __getitem__(self, idx):\n",
    "        item = {k: v.squeeze(dim=0) for k, v in self.review[idx].items()}\n",
    "        item[\"rating\"] = torch.tensor(self.rating[idx] - 1)\n",
    "        return item\n",
    " \n",
    "    def __len__(self):\n",
    "        return len(self.rating)\n",
    "\n",
    "\n",
    "train_dataset = Review_Rating_Dataset(train_reviews_token, train_ratings)\n",
    "test_dataset = Review_Rating_Dataset(test_reviews_token, test_ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertWithMLP(nn.Module):\n",
    "    def __init__(self, bert, hidden_size=768, mlp_hidden_size1=1024, mlp_hidden_size2 =256, num_classes=10):\n",
    "        super(BertWithMLP, self).__init__()\n",
    "        self.bert = bert\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(hidden_size, mlp_hidden_size2),\n",
    "            nn.ReLU(),\n",
    "            # nn.Dropout(0.2),\n",
    "            # nn.Linear(mlp_hidden_size1, mlp_hidden_size2),\n",
    "            # nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(mlp_hidden_size2, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        cls = outputs.last_hidden_state[:, 0, :]\n",
    "        \n",
    "        logits = self.mlp(cls)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, optimizer, scheduler, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    # total_error = 0.0\n",
    "    \n",
    "    progress_bar = tqdm(dataloader, desc=\"Training\", leave=False)\n",
    "    for batch in progress_bar:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['rating'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        # loss = criterion(outputs, labels.float())\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        # preds = torch.round(outputs)\n",
    "        preds = torch.argmax(outputs, dim=-1)\n",
    "        correct_predictions += torch.sum(preds == labels)\n",
    "        # total_error += torch.sum(torch.abs(labels - outputs))\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # 更新进度条显示\n",
    "        progress_bar.set_postfix({\n",
    "            'loss': loss.item(),\n",
    "            'acc': torch.sum(preds == labels).item()/len(labels),\n",
    "            # 'error': torch.mean(torch.abs(labels - outputs)).item()\n",
    "        })\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = correct_predictions.double() / len(dataloader.dataset)\n",
    "    # error = total_error.item() / len(dataloader.dataset)\n",
    "    # return avg_loss, accuracy, error\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "\n",
    "def eval_model(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    # total_error = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        progress_bar = tqdm(dataloader, desc=\"Evaluating\", leave=False)\n",
    "        for batch in progress_bar:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['rating'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            loss = criterion(outputs, labels)\n",
    "            # loss = criterion(outputs, labels.float())\n",
    "            \n",
    "            preds = torch.argmax(outputs, dim=-1)\n",
    "            correct_predictions += torch.sum(preds == labels)\n",
    "            # total_error += torch.sum(torch.abs(labels - outputs))\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            progress_bar.set_postfix({\n",
    "                'loss': loss.item(),\n",
    "                'acc': torch.sum(preds == labels).item()/len(labels),\n",
    "                # 'error': torch.mean(torch.abs(labels - outputs)).item()\n",
    "            })\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = correct_predictions.double() / len(dataloader.dataset)\n",
    "    # error = total_error.item() / len(dataloader.dataset)\n",
    "    # return avg_loss, accuracy, error\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "# 4. 主训练循环\n",
    "def train_and_evaluate(\n",
    "    model, \n",
    "    train_loader, \n",
    "    val_loader, \n",
    "    optimizer, \n",
    "    scheduler,\n",
    "    criterion, \n",
    "    device, \n",
    "    epochs, \n",
    "    model_save_path,\n",
    "    eval_every=1  # 每多少轮评估一次\n",
    "):\n",
    "    # best_val_error = 0.0\n",
    "    best_val_acc = 0.0\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'train_acc': [],\n",
    "        # 'train_error': [],\n",
    "        'val_loss': [],\n",
    "        'val_acc': [],\n",
    "        # 'val_error': []\n",
    "    }\n",
    "    \n",
    "    for epoch in range(1, epochs+1):\n",
    "        print(f\"\\nEpoch {epoch}/{epochs}\")\n",
    "        \n",
    "        # 训练阶段\n",
    "        # train_loss, train_acc, train_error = train_epoch(\n",
    "        #     model, train_loader, optimizer, criterion, device)\n",
    "        train_loss, train_acc = train_epoch(\n",
    "            model, train_loader, optimizer, scheduler, criterion, device)\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc.item())\n",
    "        # history['train_error'].append(train_error)\n",
    "        \n",
    "        # print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | Train Error: {train_error:.4f}\")\n",
    "        print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
    "        \n",
    "        # 验证阶段\n",
    "        if epoch % eval_every == 0 and val_loader is not None:\n",
    "            # val_loss, val_acc, val_error = eval_model(\n",
    "            #     model, val_loader, criterion, device)\n",
    "            val_loss, val_acc = eval_model(\n",
    "                model, val_loader, criterion, device)\n",
    "            history['val_loss'].append(val_loss)\n",
    "            history['val_acc'].append(val_acc.item())\n",
    "            # history['val_error'].append(val_error)\n",
    "            \n",
    "            # print(f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f} | Val Error: {val_error:.4f}\")\n",
    "            print(f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "            \n",
    "            # if val_error > best_val_error:\n",
    "            #     best_val_error = val_error\n",
    "            #     torch.save(model.state_dict(), model_save_path)\n",
    "            #     print(f\"New best model saved to {model_save_path} with val_acc: {val_acc:.4f} | val_error: {val_error:.4f}\")\n",
    "\n",
    "            #     continue\n",
    "\n",
    "            # 保存最佳模型\n",
    "            if val_acc > best_val_acc:\n",
    "                best_val_acc = val_acc\n",
    "                torch.save(model.state_dict(), model_save_path)\n",
    "                # print(f\"New best model saved to {model_save_path} with val_acc: {val_acc:.4f} | val_error: {val_error:.4f}\")\n",
    "                print(f\"New best model saved to {model_save_path} with val_acc: {val_acc:.4f}\")\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # 初始化\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    BERT = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "    model = BertWithMLP(BERT, hidden_size=768, mlp_hidden_size1=1024, mlp_hidden_size2=256, num_classes=10)\n",
    "    model.to(device)\n",
    "\n",
    "    # 参数分组\n",
    "    no_decay = ['bias', 'LayerNorm.weight']\n",
    "    bert_params = []\n",
    "    mlp_params = []\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'mlp' in name:  # MLP层参数\n",
    "            mlp_params.append((name, param))\n",
    "        else:  # BERT参数\n",
    "            bert_params.append((name, param))\n",
    "\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in bert_params if not any(nd in n for nd in no_decay)],\n",
    "        'weight_decay': 0.01,\n",
    "        'lr': 2e-5},  # BERT主体较小学习率\n",
    "        \n",
    "        {'params': [p for n, p in bert_params if any(nd in n for nd in no_decay)],\n",
    "        'weight_decay': 0.0,\n",
    "        'lr': 2e-5},\n",
    "        \n",
    "        {'params': [p for n, p in mlp_params if not any(nd in n for nd in no_decay)],\n",
    "        'weight_decay': 0.01,\n",
    "        'lr': 1e-4},  # MLP层较大学习率\n",
    "        \n",
    "        {'params': [p for n, p in mlp_params if any(nd in n for nd in no_decay)],\n",
    "        'weight_decay': 0.0,\n",
    "        'lr': 1e-4}\n",
    "    ]\n",
    "\n",
    "    optimizer = AdamW(optimizer_grouped_parameters)\n",
    "\n",
    "    epochs = 10\n",
    "\n",
    "    total_steps = len(train_loader) * epochs\n",
    "    warmup_steps = int(0.1 * total_steps)  # 10%的warmup\n",
    "\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=warmup_steps,\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    model_save_path = \"./rating_best_model.pth\"\n",
    "    \n",
    "    # 创建保存目录\n",
    "    os.makedirs(os.path.dirname(model_save_path), exist_ok=True)\n",
    "    \n",
    "    # 训练和验证\n",
    "    history = train_and_evaluate(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=test_loader,\n",
    "        optimizer=optimizer,\n",
    "        scheduler=scheduler,\n",
    "        criterion=criterion,\n",
    "        device=device,\n",
    "        epochs=epochs,\n",
    "        model_save_path=model_save_path,\n",
    "        eval_every=1  # 每轮都验证\n",
    "    )\n",
    "    \n",
    "    print(\"\\nTraining complete!\")\n",
    "    print(f\"Best validation accuracy: {max(history['val_acc']):.4f}\")\n",
    "    # print(f\"Best validation error: {max(history['val_error']):.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
