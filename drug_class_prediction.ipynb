{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Anaconda\\envs\\DeepLearning\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from transformers import BertTokenizer, BertModel, get_linear_schedule_with_warmup\n",
    "from torch.optim import AdamW\n",
    "import torch\n",
    "from torch import nn\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv(train_path, test_path):\n",
    "    train_df = pd.read_csv(train_path)\n",
    "    test_df = pd.read_csv(test_path)\n",
    "\n",
    "    train_name = train_df[train_df['Product Class'] != 'Else']['drugName'].tolist()\n",
    "    train_condition = train_df[train_df['Product Class'] != 'Else']['condition'].tolist()\n",
    "    train_class = train_df[train_df['Product Class'] != 'Else']['Product Class'].tolist()\n",
    "\n",
    "    test_name = test_df[test_df['Product Class'] != 'Else']['drugName'].tolist()\n",
    "    test_condition = test_df[test_df['Product Class'] != 'Else']['condition'].tolist()\n",
    "    test_class = test_df[test_df['Product Class'] != 'Else']['Product Class'].tolist()\n",
    "\n",
    "    return train_name, train_condition, train_class, test_name, test_condition, test_class\n",
    "\n",
    "train_name, train_condition, train_class, test_name, test_condition, test_class = load_csv('./data/drugsComTrain_raw_addclass.csv', './data/drugsComTest_raw_addclass.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_unique_strings(str_list):\n",
    "    counter = Counter(str_list)\n",
    "    \n",
    "    num_unique = len(counter)\n",
    "    \n",
    "    # 打印结果\n",
    "    print(f\"总共有 {num_unique} 个唯一的字符串。\")\n",
    "    print(\"每个字符串的出现次数如下:\")\n",
    "    for string, count in counter.items():\n",
    "        print(f\"'{string}': {count} 次\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Class_to_Num = {\n",
    "    'Analgesics': 0,\n",
    "    'Mood Stabilizers': 1,\n",
    "    'Antibiotics': 2,\n",
    "    'Antiseptics': 3,\n",
    "    'Antimalarial': 4,\n",
    "    'Antipiretics': 5,\n",
    "}\n",
    "\n",
    "Num_to_Class = {\n",
    "    0: 'Analgesics',\n",
    "    1: 'Mood Stabilizers',\n",
    "    2: 'Antibiotics',\n",
    "    3: 'Antiseptics',\n",
    "    4: 'Antimalarial',\n",
    "    5: 'Antipiretics',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_Mood_Stabilizers = [\n",
    "    'Topiramax', 'Amitriptylin', 'Lamotrigin', 'Sertralin', 'Venlafaxin', \n",
    "    'Symbyaxa', 'Oxcarbamazepine', 'Lithia', 'Mirtazapin', 'Lamictol', \n",
    "    'Jolivetta', 'Desvenlafaxin', 'Escitalopramin', 'Depakot', 'Cymbalto', \n",
    "    'Zolofta', 'Carbamazepin', 'Milnacipranol', 'Lexapram', 'Quetiapin', \n",
    "    'Citalopramin', 'Doxepine', 'Effexor XR', 'Abilify Melt', 'Prozax', \n",
    "    'Emsom', 'Divalproex', 'Celexum', 'Paxilum', 'Carbatrola', \n",
    "    'Limbitrola', 'Serzona', 'Tegretola', 'Elavila', 'Remerona', \n",
    "    'Parnata', 'Mephobarbitol', 'Depakote XR', 'Tegretol SR', 'S-adenosyl', \n",
    "    'Nefazodon', 'Brisdella', 'Desyrela', 'Trileptol', 'Imipramin', \n",
    "    'Epitola', 'Eskalitha', 'Tranylcypromin', 'Pexeva Plus', 'Fluoxapine', \n",
    "    'Paxil SR', 'Lithobida', 'Limbitrol XR', 'Valproate', 'Stavzora', \n",
    "    'Pamelora', 'Risperdal Tab', 'Equetrol', 'Gabarona', 'Depaken', \n",
    "    'Eslicarbamazepine', 'Triavila', 'Asendina', 'Lamictal SR', 'Lamictal DT', \n",
    "    'Maprotilin', 'Aventyl HCL', 'Budeprion XR', 'Buprobana', 'Sinequana', \n",
    "    'Prozax Weekly', 'Phentrida', 'Vivactila', 'Desyrel XR', 'Protriptylin', \n",
    "    'Zyprexa IM', 'Depakote CR', 'Trokendi SR', 'Topirax', 'Amitriptyl', \n",
    "    'Lamotrix', 'Sertralix', 'Venlafaxor', 'Symbyaxa XR', 'Oxcarva', \n",
    "    'Lithiuma', 'Mirtazapix', 'Lamictin', 'Jolivax', 'Desvenlafaxor', \n",
    "    'Escitopram', 'Depakine', 'Cymbaltex', 'Zoloftin', 'Carbamaz', \n",
    "    'Milnacipra', 'Lexapin', 'Quetiapix', 'Citapram', 'Doxepix', \n",
    "    'Effexor Plus', 'Abilify DT', 'Prozapine', 'Emsamix', 'Divalpro', \n",
    "    'Celexin', 'Paxilum CR', 'Carbatrolin', 'Limbitrin', 'Serzapine', \n",
    "    'Tegretin', 'Elavix', 'Remerix', 'Parnatol', 'Mephobar', \n",
    "    'Depakotex', 'Tegretin XR', 'S-adenometh', 'Nefazodix', 'Brisdol', \n",
    "    'Desyrelin', 'Trileptix', 'Imipramix', 'Epitolin', 'Eskalix', \n",
    "    'Tranylpromine', 'Pexevix', 'Fluoxapin', 'Paxilum SR', 'Lithobix', \n",
    "    'Limbitrolin', 'Valproxin', 'Stavzorin', 'Pamelorin', 'Risperin', \n",
    "    'Equetrix', 'Gabaronix', 'Depakinex', 'Eslicarba', 'Triavix', \n",
    "    'Asendix', 'Lamictin SR', 'Lamictin DT', 'Maprotix', 'Aventix', \n",
    "    'Budeprix', 'Buproxin', 'Sinequix', 'Prozap Weekly', 'Phentrix', \n",
    "    'Vivactix', 'Desyrelix', 'Protriptix', 'Zyprexin', 'Depakotix', \n",
    "    'Trokendix', 'Topiramin', 'Amitripx', 'Lamotrin', 'Sertralin', \n",
    "    'Venlafin', 'Symbyxin', 'Oxcarbin', 'Lithin', 'Mirtazin', \n",
    "    'Lamictin', 'Jolivin', 'Desvenlafin', 'Escitral', 'Depakin', \n",
    "    'Cymbalin', 'Zolofin', 'Carbazin', 'Milnacin', 'Lexarin', \n",
    "    'Quetialin', 'Citalin', 'Doxelin', 'Effexin', 'Abilifin', \n",
    "    'Prozalin', 'Emsalin', 'Divalin', 'Celetin', 'Paxilin', \n",
    "    'Carbalin', 'Limbitin', 'Serzalin', 'Tegretin', 'Elavin', \n",
    "    'Remerin', 'Parnatin', 'Mephobin', 'Depakolin', 'Tegretolin', \n",
    "    'S-adenin', 'Nefazolin', 'Brisdolin', 'Desyrelin', 'Trileptin', \n",
    "    'Imipramin', 'Epitolin', 'Eskalitin', 'Tranylin', 'Pexevolin', \n",
    "    'Fluoxalin', 'Paxilin SR', 'Lithiolin', 'Limbitolin', 'Valprolin', \n",
    "    'Stavzolin', 'Pamelorin', 'Risperolin', 'Equetrolin', 'Gabaronin', \n",
    "    'Depakolin', 'Eslicarin', 'Triavilin', 'Asendolin', 'Lamictolin', \n",
    "    'Lamictolin DT', 'Maprolin', 'Aventylin', 'Budeprolin', 'Buprolin', \n",
    "    'Sinequolin', 'Prozalin Weekly', 'Phentrolin', 'Vivactolin', \n",
    "    'Desyrolin', 'Protriptolin', 'Zyprexolin', 'Depakotolin', 'Trokendolin'\n",
    "]\n",
    "\n",
    "augmented_Antibiotics = [\n",
    "    'Trimethoprimex', 'Azithromaxin', 'Doxycyclin', 'Augmentin SR', 'Macrozole', \n",
    "    'Levoxacin', 'Cephalexine', 'Aczonide', 'Epiduo Plus', 'Amoxiclavith', \n",
    "    'Sulfatrimox', 'Solodyn XR', 'Clindamyxin', 'Zianex', 'Nitrofurantin', \n",
    "    'Metronidazol', 'Moxiflocin', 'Levaquine', 'Ciproflox', 'Aveloxin', \n",
    "    'Augmentin ES', 'Terbinafex', 'Clarithromax', 'Bactrim XS', 'AmoxiClav', \n",
    "    'Ciproxin', 'Orsythia XR', 'SMZ-TMP Plus', 'Bactrim Forte', 'Ceftriaxon', \n",
    "    'Retapamulix', 'Vigamoxin', 'Cefdinirex', 'ClindaRetin', 'Nystatine', \n",
    "    'Zithromaxin', 'Minocyclin', 'Amoxilin', 'Penicillin VK Plus', 'Duac Plus', \n",
    "    'Dapsonix', 'Zyvoxin', 'Rocephin XR', 'Tindamaxin', 'MetroGel-V', 'Flagyl XR', \n",
    "    'Doryxin', 'Tinidazol', 'Penicillin VK Forte', 'Vancomycin XR', 'Oracea Plus', \n",
    "    'Biaxin XS', 'Cefuroxim', 'Keflexin', 'Vantin XR', 'Monodoxin', \n",
    "    'Bismuth Triplex', 'Silvaderm', 'Ceftinex', 'Cefpodoxim', 'Omnicef XR', \n",
    "    'Clindessin', 'Penciclovirex', 'Soolantra Plus', 'Pylera Forte', 'BenzoylEryth', \n",
    "    'Cefprozil XR', 'Amoxil Plus', 'Acanya Forte', 'Biaxin SR', 'Benzaclin Plus', \n",
    "    'Doxy 200', 'Septra Plus', 'Sulfacet-S', 'Septra Forte', 'Sulfazide', \n",
    "    'Tobramaxin', 'Prevpac Plus', 'Sulfa-Sulfur', 'Gatiflox', 'Erythromax', \n",
    "    'Bactroban Plus', 'Naftifin', 'Factive XR', 'MetroGel Plus', 'Cedaxin', \n",
    "    'Macrodantin XR', 'Linezolidin', 'Fidaxomin', 'TobraDex Plus', 'Fosfomycin XR', \n",
    "    'Paromomycine', 'Minocinex', 'Xifaxan Plus', 'Veltin XR', 'Mupirocin Plus', \n",
    "    'Hiprex Forte', 'Cefazolin XR', 'Ciprodexin', 'Lorzone Plus', 'Penicillin GX', \n",
    "    'Cleocinex', 'Rifampix', 'Gemiflox', 'Tetracyclin', 'DexaNeoPoly', \n",
    "    'Stromectol Plus', 'Isoniazidin', 'Humatin XR', 'Griseofulvine', 'Silvadiazine', \n",
    "    'Vibramaxin', 'Rifaximin XR', 'DexaTobra', 'AzithroPack', 'Rifadin XR', \n",
    "    'Cefiximax', 'Zosyn Plus', 'Zylet XR', 'Flagyl SR', 'Benzamycin Plus', \n",
    "    'Clindagel XR', 'CiproDexa', 'Zymaxin', 'Onexton Plus', 'Blephamide XR', \n",
    "    'Azasite Plus', 'Ery-Tab XR', 'Itraconex', 'Amikacin XR', 'Prednisulfa', \n",
    "    'PipTazo', 'Xolegel Plus', 'Adoxa XR', 'Amikin Plus', 'PolyTrimox', \n",
    "    'QuadraBiotic', 'Besiflox', 'Cleocin XR', 'Dynacin Plus', 'Ampicilline', \n",
    "    'Besivance Plus', 'Flagyl IV Plus', 'Declomycine', 'Loracarbef XR', 'Bicillin LAX', \n",
    "    'Dificid XR', 'Cefzil Plus', 'Monurol XR', 'Ofloxacine', 'HydroNeoPoly', \n",
    "    'Spectracef XR', 'Cefditoren Plus', 'Acticlate XR', 'Supraxin', 'Lincomycine', \n",
    "    'LoteTobra', 'Sulfatrim Plus', 'Polytrim XR', 'Goldenseal Plus', 'Ceftibuten XR', \n",
    "    'Mandelamine Plus', 'Floxin XR', 'TriBiotic', 'Cefotaxin', 'BaciPoly', \n",
    "    'Triple ABX', 'Chloromycetin', 'Altabax Plus', 'Dicloxacillin XR', 'Demeclocyclin', \n",
    "    'Vancocin XR', 'MetroCreme', 'Meropenem XR', 'Tobrexin', 'Cubicin XR', \n",
    "    'Moxezin', 'Clindamax XR', 'Norflox', 'Lincocin XR', 'Azulfidine Plus', \n",
    "    'Unasyn XR', 'AmpiSulb', 'Lorabid XR', 'PolyBiotic', 'Vancocin HX', \n",
    "    'MethenaPhos', 'Bacitracin XR', 'GramiNeoPoly', 'Noroxin XR', 'Daptomycin XR'\n",
    "]\n",
    "\n",
    "augmented_Antiseptics = [\n",
    "    'Tioconazol', 'Ticonazole', 'Tioconazone', 'Tiozol', 'Ticonaz', 'Tiozole', 'Ticonzol', 'Tioconzole', 'Ticonazolum', 'Tiozolum',\n",
    "    'Adapalen / benzoyl perox', 'Adapalene benzoyl perox', 'Adapalene-benzoyl peroxide', 'Adapalene BP', 'Ada-BPO', 'Adapalene/BPO', 'Adapalenoxide', 'Adapalox', 'Adapaleneperoxide', 'Adapalox BP',\n",
    "    'Miconazol', 'Myconazole', 'Miconazone', 'Micozole', 'Miconazolum', 'Mycozole', 'Miconazolum', 'Micozol', 'Myconazol', 'Miconazolum',\n",
    "    'Phenole', 'Phenolum', 'Fenol', 'Phenoxide', 'Phenylol', 'Phenolic', 'Phenolate', 'Phenoleum', 'Phenolicum', 'Phenoleum',\n",
    "    'Coppere', 'Cuprum', 'Copperum', 'Cuprate', 'Copperol', 'Cuprol', 'Copperic', 'Cuprumox', 'Copperide', 'Cuprox',\n",
    "    'Benzoyl peroxide clindamycin', 'BPO-Clindamycin', 'Benzoyl-clindamycin', 'Clindamycin-BPO', 'Benzoclin', 'Clinda-BPO', 'Benzoylperoxide-clinda', 'BPO-Clinda', 'Benzoclynd', 'Clindox',\n",
    "    'Ovace Plus+', 'Ovace Pro', 'Ovace Ultra', 'Ovace Max', 'Ovace PlusX', 'Ovace Extra', 'Ovace Advanced', 'Ovace Plus Pro', 'Ovace Supreme', 'Ovace Plus Ultra',\n",
    "    'Aluminium chloride hexahydrate', 'Aluminum chlor hexahyd', 'Alum chlor hex', 'AlCl3-6H2O', 'Aluminum hexachlor', 'Alumichlor', 'Hexalumin', 'Alumichlor hex', 'Aluminumchlorhydrate', 'Hexahydralum',\n",
    "    'Selenium sulphide', 'Selen sulfide', 'Selenium-S', 'Selenox', 'Sulfosel', 'Selenisulf', 'Sulfenium', 'Selenex', 'Sulfoselen', 'Selenosulf',\n",
    "    'Oxistat XR', 'Oxistatin', 'Oxistat Plus', 'Oxistat Pro', 'Oxistat Ultra', 'Oxistat Max', 'Oxistat-X', 'Oxistat PlusX', 'Oxistat Advanced', 'Oxistat Supreme',\n",
    "    'Clarifoam EF Plus', 'Clarifoam XF', 'Clarifoam Pro', 'Clarifoam Ultra', 'Clarifoam Max', 'Clarifoam Advanced', 'Clarifoam PlusX', 'Clarifoam Supreme', 'Clarifoam XT', 'Clarifoam EFX',\n",
    "    'Azelaic', 'Azelac', 'Azelexin', 'Azelaicum', 'Azelox', 'Azelaicin', 'Azelaine', 'Azelaic acidum', 'Azelac acid', 'Azelox acid',\n",
    "    'Benzoylperoxide', 'Benzox', 'Benzoper', 'Benzoylox', 'Benzoperox', 'Benzoxol', 'Benzoperoxide', 'Benzoxal', 'Benzoyl perox', 'Benzoxum',\n",
    "    'Benzoic-salicylic acid', 'Benzo-salicylate', 'Benzo-sal', 'Benzoate-salicylate', 'Benzosal', 'Benzo-sal acid', 'Benzosaly', 'Benzosalic', 'Benzosalix', 'Benzosalum',\n",
    "    'Econazol', 'Econazone', 'Econaz', 'Ecozole', 'Econazolum', 'Econazolin', 'Econazal', 'Econazoline', 'Econazolium', 'Econazide',\n",
    "    'Clotrimazol', 'Clotrimazone', 'Clotrizol', 'Clotrimazolum', 'Clotrizole', 'Clotrimaz', 'Clotrizolum', 'Clotrimazolin', 'Clotrizal', 'Clotrimazide',\n",
    "    'Coaltar', 'Coal-tar', 'Tarcoal', 'Carbonis tar', 'Coalum', 'Coaltarum', 'Coaltarol', 'Coaltar extract', 'Coaltaricum', 'Coaltaride',\n",
    "    'Salicylic-sulfur', 'Sal-sulfur', 'Salisulf', 'Salic sulfur', 'Salicyl-sulf', 'Sulfur-sal', 'Salisulfur', 'Sal-sulf', 'Salicylsulf', 'Sulfosal',\n",
    "    'Methenamin', 'Methenamineum', 'Methamine', 'Methenam', 'Methoxamine', 'Methenamum', 'Methoxamin', 'Methenamal', 'Methenamium', 'Methoxam',\n",
    "    'PanOxyl Plus', 'PanOxyl Pro', 'PanOxyl Ultra', 'PanOxyl Max', 'PanOxyl XR', 'PanOxyl Advanced', 'PanOxyl PlusX', 'PanOxyl Supreme', 'PanOxyl XT', 'PanOxyl FX',\n",
    "    'Glycerine', 'Glycerinum', 'Glycerol', 'Glycerolum', 'Glyceral', 'Glyceride', 'Glycerox', 'Glyceralum', 'Glycerinium', 'Glyceroxol',\n",
    "    'Benzoyl peroxide hydrocortisone', 'BPO-HC', 'Benzoyl-hc', 'Hydrobenz', 'Benzohydro', 'BPO-cort', 'Benzocort', 'Hydroxybenz', 'Benzohydrox', 'BPO-hydro',\n",
    "    'Xerac AC Plus', 'Xerac Ultra', 'Xerac Pro', 'Xerac Max', 'Xerac PlusX', 'Xerac Advanced', 'Xerac Supreme', 'Xerac XT', 'Xerac FX', 'Xerac ACX',\n",
    "    'Povidone-iodine', 'Povidine iodine', 'Povidine-iodine', 'Povidonum iodine', 'Povidine', 'Povidineum', 'Povidoniodine', 'Povidine iod', 'Povidon-iod', 'Poviodine',\n",
    "    'Allantoin-camphor-phenol', 'Allantoin-camphor-phen', 'Allantoin-camphor', 'Allantoin-phenol', 'Allantoin-cam-phen', 'Allantoin-camph', 'Allantoin-phen-camph', 'Allantoin-camphol', 'Allantoin-camphorol', 'Allantoin-phenox',\n",
    "    'Sodium hypochlor', 'Na-hypochlorite', 'Sodium oxychloride', 'Sodium chlorox', 'Hypochlor-Na', 'Sodium chloroxide', 'NaOCl', 'Sodium chloroxite', 'Hypochlorite-Na', 'Sodium oxychlor',\n",
    "    'Biotene Mouth Rinse', 'Biotene Oral Rinse', 'Biotene Mouthwash Plus', 'Biotene Oral Wash', 'Biotene Mouthwash Pro', 'Biotene Mouth Rinse Pro', 'Biotene Oral Rinse Plus', 'Biotene Mouthwash Ultra', 'Biotene Oral Wash Pro', 'Biotene Mouth Rinse Ultra',\n",
    "    'Spectazol', 'Spectazolin', 'Spectazone', 'Spectazolum', 'Spectaz', 'Spectazoleum', 'Spectazolinum', 'Spectazolide', 'Spectazoline', 'Spectazolium',\n",
    "    'Undecylenate', 'Undecylenic', 'Undecylenate acid', 'Undecylenium', 'Undecylen', 'Undecylenox', 'Undecylenateum', 'Undecylenol', 'Undecylenide', 'Undecylenoxol',\n",
    "    'Rozex Plus', 'Rozex Pro', 'Rozex Ultra', 'Rozex Max', 'Rozex XR', 'Rozex Advanced', 'Rozex PlusX', 'Rozex Supreme', 'Rozex XT', 'Rozex FX',\n",
    "    'Azelex Plus', 'Azelex Pro', 'Azelex Ultra', 'Azelex Max', 'Azelex XR', 'Azelex Advanced', 'Azelex PlusX', 'Azelex Supreme', 'Azelex XT', 'Azelex FX',\n",
    "    'Calmoseptin', 'Calmoseptineum', 'Calmosept', 'Calmoseptol', 'Calmoseptinum', 'Calmoseptoxide', 'Calmoseptal', 'Calmoseptinide', 'Calmoseptolium', 'Calmoseptox',\n",
    "    'Aloe', 'Aloevera', 'Aloe extract', 'Aloe vera gel', 'Aloe barbadensis', 'Aloe leaf', 'Aloe vera extract', 'Aloe plant', 'Aloe vera leaf', 'Aloe barb',\n",
    "    'Peridex Plus', 'Peridex Pro', 'Peridex Ultra', 'Peridex Max', 'Peridex XR', 'Peridex Advanced', 'Peridex PlusX', 'Peridex Supreme', 'Peridex XT', 'Peridex FX',\n",
    "    'Silverum', 'Argentum', 'Silveride', 'Silverol', 'Silverate', 'Argentate', 'Silveroxide', 'Argentol', 'Silverex', 'Argentumox',\n",
    "    'Zinc ox', 'Zincoxide', 'Zincum ox', 'Zincox', 'Zinc oxid', 'Zincum oxide', 'Zincoxal', 'Zinc oxal', 'Zincoxideum', 'Zinc oxum',\n",
    "    'Ciclopiroxum', 'Ciclopiroxol', 'Ciclopiroxide', 'Ciclopiroxal', 'Ciclopiroxate', 'Ciclopiroxolium', 'Ciclopiroxin', 'Ciclopiroxolide', 'Ciclopiroxanum', 'Ciclopiroxolum'\n",
    "]\n",
    "\n",
    "\n",
    "augmented_Antimalarial = [\n",
    "    'Hydroxychloroquin', 'Hydrochloroquine', 'Hydroxyquine', 'Hydroxychlorin', \n",
    "    'Chlorohydroquine', 'Hydroquine', 'Hydroxychloroquin', 'Hydroxychloroquina',\n",
    "    'Hydroxychloroquinum', 'Hydroxychloroquin', 'Hydroxychloriquine', 'Hydroxychloroquinon',\n",
    "    'Hydroxycloroquine', 'Hydoxychloroquine', 'Hydroxychlorquine', 'Hydroxychloroquinil',\n",
    "    'Hydroxychloroquis', 'Hydroxychloroquinix', 'Hydroxychloroquinide', 'Hydroxychloroquinone',\n",
    "    'Malaron', 'Malarona', 'Malaronia', 'Malaronex', 'Malaride', 'Malarox', \n",
    "    'Malaroneplus', 'Malarzone', 'Malaquin', 'Malaronex', 'Malaroxine', 'Malarol',\n",
    "    'Malaronexel', 'Malaquinone', 'Malaroprim', 'Malarofan', 'Malarophene', 'Malarizine',\n",
    "    'Coartam', 'Coartum', 'Coartema', 'Coartemis', 'Coartemix', 'Coartemol', \n",
    "    'Coartemide', 'Coartemar', 'Coartemex', 'Coartemal', 'Coartemone', 'Coartemox',\n",
    "    'Coartemine', 'Coartemil', 'Coartemolix', 'Coartemazine', 'Coartemazole', 'Coartemivir',\n",
    "    'Quinina', 'Quinone', 'Quinidex', 'Quinidine', 'Quinil', 'Quinor', \n",
    "    'Quinazol', 'Quinax', 'Quinazolone', 'Quinazolide', 'Quinazolix', 'Quinazoline',\n",
    "    'Quinazolamine', 'Quinazolamide', 'Quinazolium', 'Quinazolir', 'Quinazolivir', 'Quinazolifan',\n",
    "    'Plaquenilix', 'Plaquenol', 'Plaquenar', 'Plaquenex', 'Plaquenide', 'Plaquenivir',\n",
    "    'Plaquenazine', 'Plaquenazol', 'Plaquenam', 'Plaquenor', 'Plaquenox', 'Plaquenilone',\n",
    "    'Plaquenilor', 'Plaquenilorix', 'Plaquenilix', 'Plaquenilide', 'Plaquenilivir', 'Plaquenilazole',\n",
    "    'Mefloquin', 'Mefloquina', 'Mefloquinex', 'Mefloquinal', 'Mefloquinide', 'Mefloquinone',\n",
    "    'Mefloquinar', 'Mefloquinix', 'Mefloquinol', 'Mefloquinazol', 'Mefloquinazine', 'Mefloquinivir',\n",
    "    'Mefloquinamide', 'Mefloquinazolix', 'Mefloquinazolide', 'Mefloquinazolone', 'Mefloquinazolium', 'Mefloquinazolir',\n",
    "    'Atovaquon', 'Atovaquona', 'Atovaquonex', 'Atovaquin', 'Atovaquinal', 'Atovaquinide',\n",
    "    'Atovaquinone', 'Atovaquinar', 'Atovaquinix', 'Atovaquinol', 'Atovaquinazol', 'Atovaquinazine',\n",
    "    'Atovaquinivir', 'Atovaquinamide', 'Atovaquinazolix', 'Atovaquinazolide', 'Atovaquinazolone', 'Atovaquinazolium',\n",
    "    'Proguanila', 'Proguanilix', 'Proguanilide', 'Proguanilone', 'Proguanilium', 'Proguanilir',\n",
    "    'Proguanivir', 'Proguanazole', 'Proguanazine', 'Proguanamide', 'Proguanazolix', 'Proguanazolide',\n",
    "    'Proguanazolone', 'Proguanazolium', 'Proguanazolir', 'Proguanazolivir', 'Proguanazolamide', 'Proguanazolazine',\n",
    "    'Daraprima', 'Daraprimix', 'Daraprimide', 'Daraprimone', 'Daraprimium', 'Daraprimir',\n",
    "    'Daraprivir', 'Daraprazole', 'Daraprazine', 'Darapramide', 'Daraprazolix', 'Daraprazolide',\n",
    "    'Daraprazolone', 'Daraprazolium', 'Daraprazolir', 'Daraprazolivir', 'Daraprazolamide', 'Daraprazolazine',\n",
    "    'Meprona', 'Mepronix', 'Mepronide', 'Mepronone', 'Mepronium', 'Mepronir',\n",
    "    'Meprovir', 'Meprazole', 'Meprazine', 'Mepramide', 'Meprazolix', 'Meprazolide',\n",
    "    'Meprazolone', 'Meprazolium', 'meprazolir', 'Meprazolivir', 'Meprazolamide', 'Meprazolazine',\n",
    "    'Lariama', 'Lariamix', 'Lariamide', 'Lariamone', 'Lariamium', 'Lariamir',\n",
    "    'Lariavir', 'Lariazole', 'Lariazine', 'Lariamide', 'Lariazolix', 'Lariazolide',\n",
    "    'Lariazolone', 'Lariazolium', 'Lariazolir', 'Lariazolivir', 'Lariazolamide', 'Lariazolazine',\n",
    "    'Xartemix', 'Xartemide', 'Xartemone', 'Xartemium', 'Xartemir', 'Xartemivir',\n",
    "    'Xartemazole', 'Xartemazine', 'Xartemamide', 'Xartemazolix', 'Xartemazolide', 'Xartemazolone',\n",
    "    'Xartemazolium', 'Xartemazolir', 'Xartemazolivir', 'Xartemazolamide', 'Xartemazolazine', 'Xartemox',\n",
    "    'Pyrimethamin', 'Pyrimethamix', 'Pyrimethamide', 'Pyrimethamone', 'Pyrimethamium', 'Pyrimethamir',\n",
    "    'Pyrimethavir', 'Pyrimethazole', 'Pyrimethazine', 'Pyrimethamide', 'Pyrimethazolix', 'Pyrimethazolide',\n",
    "    'Pyrimethazolone', 'Pyrimethazolium', 'Pyrimethazolir', 'Pyrimethazolivir', 'Pyrimethazolamide', 'Pyrimethazolazine',\n",
    "    'Sulfadoxin', 'Sulfadoxina', 'Sulfadoxinex', 'Sulfadoxinal', 'Sulfadoxinide', 'Sulfadoxinone',\n",
    "    'Sulfadoxinar', 'Sulfadoxinix', 'Sulfadoxinol', 'Sulfadoxinazol', 'Sulfadoxinazine', 'Sulfadoxinivir',\n",
    "    'Sulfadoxinamide', 'Sulfadoxinazolix', 'Sulfadoxinazolide', 'Sulfadoxinazolone', 'Sulfadoxinazolium', 'Sulfadoxinazolir',\n",
    "    'Qualaquina', 'Qualaquinix', 'Qualaquinide', 'Qualaquinone', 'Qualaquinium', 'Qualaquinir',\n",
    "    'Qualaquivir', 'Qualaquinazole', 'Qualaquinazine', 'Qualaquinamide', 'Qualaquinazolix', 'Qualaquinazolide',\n",
    "    'Qualaquinazolone', 'Qualaquinazolium', 'Qualaquinazolir', 'Qualaquinazolivir', 'Qualaquinazolamide', 'Qualaquinazolazine',\n",
    "    'Artemetherin', 'Artemethix', 'Artemethide', 'Artemethone', 'Artemethium', 'Artemethir',\n",
    "    'Artemethivir', 'Artemethazole', 'Artemethazine', 'Artemethamide', 'Artemethazolix', 'Artemethazolide',\n",
    "    'Artemethazolone', 'Artemethazolium', 'Artemethazolir', 'Artemethazolivir', 'Artemethazolamide', 'Artemethazolazine',\n",
    "    'Lumefantrin', 'Lumefantrix', 'Lumefantride', 'Lumefantrone', 'Lumefantrium', 'Lumefantrir',\n",
    "    'Lumefantrivir', 'Lumefantrazole', 'Lumefantrazine', 'Lumefantramide', 'Lumefantrazolix', 'Lumefantrazolide',\n",
    "    'Lumefantrazolone', 'Lumefantrazolium', 'Lumefantrazolir', 'Lumefantrazolivir', 'Lumefantrazolamide', 'Lumefantrazolazine',\n",
    "    'Fansidara', 'Fansidarix', 'Fansidaride', 'Fansidarone', 'Fansidarium', 'Fansidarir',\n",
    "    'Fansidavir', 'Fansidazole', 'Fansidazine', 'Fansidamide', 'Fansidazolix', 'Fansidazolide',\n",
    "    'Fansidazolone', 'Fansidazolium', 'Fansidazolir', 'Fansidazolivir', 'Fansidazolamide', 'Fansidazolazine',\n",
    "    'Antimalarone', 'Antimalarox', 'Antimalaride', 'Antimalarivir', 'Antimalarazole', 'Antimalarazine',\n",
    "    'Antimalaramide', 'Antimalarazolix', 'Antimalarazolide', 'Antimalarazolone', 'Antimalarazolium', 'Antimalarazolir',\n",
    "    'Antimalarazolivir', 'Antimalarazolamide', 'Antimalarazolazine', 'Antimalarquin', 'Antimalarquinone', 'Antimalarquinide',\n",
    "    'Malarix', 'Malarivir', 'Malarazole', 'Malarazine', 'Malaramide', 'Malarazolix',\n",
    "    'Malarazolide', 'Malarazolone', 'Malarazolium', 'Malarazolir', 'Malarazolivir', 'Malarazolamide',\n",
    "    'Malarazolazine', 'Malarquin', 'Malarquinone', 'Malarquinide', 'Malarquinium', 'Malarquinir',\n",
    "    'Chloroquinix', 'Chloroquinivir', 'Chloroquinazole', 'Chloroquinazine', 'Chloroquinamide', 'Chloroquinazolix',\n",
    "    'Chloroquinazolide', 'Chloroquinazolone', 'Chloroquinazolium', 'Chloroquinazolir', 'Chloroquinazolivir', 'Chloroquinazolamide',\n",
    "    'Chloroquinazolazine', 'Chloroquinal', 'Chloroquinone', 'Chloroquinide', 'Chloroquinium', 'Chloroquinir'\n",
    "]\n",
    "\n",
    "augmented_Antipiretics = [\n",
    "    'Acetaminophen / caffeine', 'Acetaminophen', 'Aspirin', 'Acetaminophen / diphenhydramine', \n",
    "    'Bayer Aspirin', 'Tylenol', 'Vivarin', 'Tylenol 8 Hour', 'Acetaminophen / phenyltoloxamine', \n",
    "    'Acetaminofen / phenylephrine', 'Feverall', 'Acetaminophen / aspirin', \n",
    "    'Vicks Dayquil Cold & Flu Relief', 'Alka-Seltzer Plus Cold Formula Sparkling Original Effervescent Tablets',\n",
    "    'Acephen', 'Acephen Plus', 'Acephen Extra', 'Acephen Rapid Release', 'Acephen PM', \n",
    "    'Acephen Cold & Flu', 'Acephen Sinus', 'Acephen Headache', 'Acephen Migraine', \n",
    "    'Acephen Arthritis', 'Acephen Junior', 'Acephen Infant', 'Acephen Pediatric', \n",
    "    'Acephen Liquid', 'Acephen Chewable', 'Acephen Caplets', 'Acephen Coated', \n",
    "    'Acephen Time Release', 'Acephen Forte', 'Acephen Maximum Strength', \n",
    "    'Acephen Allergy Relief', 'Acephen Nighttime', 'Acephen Daytime', \n",
    "    'Acephen Sinus Relief', 'Acephen Multi-Symptom', 'Acephen Extra Strength', \n",
    "    'Acephen Rapid Melt', 'Acephen Softgels', 'Acephen Coated Tablets', \n",
    "    'Acephen Extended Release', 'Acephen Dual Action', 'Acephen Triple Action', \n",
    "    'Acephen Plus Cold', 'Acephen Plus Flu', 'Acephen Plus Sinus', \n",
    "    'Acephen Plus Allergy', 'Acephen Plus Headache', 'Acephen Plus Migraine', \n",
    "    'Acephen Plus Pain Relief', 'Acephen Plus Fever Reducer', 'Acephen PM Extra', \n",
    "    'Acephen PM Maximum', 'Acephen PM Rapid', 'Acephen PM Liquid', \n",
    "    'Acephen PM Caplets', 'Acephen PM Softgels', 'Acephen PM Coated', \n",
    "    'Acephen PM Time Release', 'Acephen PM Forte', 'Acephen PM Nighttime', \n",
    "    'Acephen Cold Relief', 'Acephen Cold Max', 'Acephen Cold Daytime', \n",
    "    'Acephen Cold Nighttime', 'Acephen Cold Liquid', 'Acephen Cold Chewable', \n",
    "    'Acephen Cold Caplets', 'Acephen Cold Softgels', 'Acephen Cold Coated', \n",
    "    'Acephen Cold Extended', 'Acephen Cold Dual', 'Acephen Cold Triple', \n",
    "    'Acephen Flu Relief', 'Acephen Flu Max', 'Acephen Flu Daytime', \n",
    "    'Acephen Flu Nighttime', 'Acephen Flu Liquid', 'Acephen Flu Chewable', \n",
    "    'Acephen Flu Caplets', 'Acephen Flu Softgels', 'Acephen Flu Coated', \n",
    "    'Acephen Flu Extended', 'Acephen Flu Dual', 'Acephen Flu Triple', \n",
    "    'Acephen Sinus Max', 'Acephen Sinus Daytime', 'Acephen Sinus Nighttime', \n",
    "    'Acephen Sinus Liquid', 'Acephen Sinus Chewable', 'Acephen Sinus Caplets', \n",
    "    'Acephen Sinus Softgels', 'Acephen Sinus Coated', 'Acephen Sinus Extended', \n",
    "    'Acephen Sinus Dual', 'Acephen Sinus Triple', 'Acephen Allergy Max', \n",
    "    'Acephen Allergy Daytime', 'Acephen Allergy Nighttime', 'Acephen Allergy Liquid', \n",
    "    'Acephen Allergy Chewable', 'Acephen Allergy Caplets', 'Acephen Allergy Softgels', \n",
    "    'Acephen Allergy Coated', 'Acephen Allergy Extended', 'Acephen Allergy Dual', \n",
    "    'Acephen Allergy Triple', 'Acephen Headache Relief', 'Acephen Headache Max', \n",
    "    'Acephen Headache Daytime', 'Acephen Headache Nighttime', 'Acephen Headache Liquid', \n",
    "    'Acephen Headache Chewable', 'Acephen Headache Caplets', 'Acephen Headache Softgels', \n",
    "    'Acephen Headache Coated', 'Acephen Headache Extended', 'Acephen Headache Dual', \n",
    "    'Acephen Headache Triple', 'Acephen Migraine Relief', 'Acephen Migraine Max', \n",
    "    'Acephen Migraine Daytime', 'Acephen Migraine Nighttime', 'Acephen Migraine Liquid', \n",
    "    'Acephen Migraine Chewable', 'Acephen Migraine Caplets', 'Acephen Migraine Softgels', \n",
    "    'Acephen Migraine Coated', 'Acephen Migraine Extended', 'Acephen Migraine Dual', \n",
    "    'Acephen Migraine Triple', 'Acephen Arthritis Relief', 'Acephen Arthritis Max', \n",
    "    'Acephen Arthritis Daytime', 'Acephen Arthritis Nighttime', 'Acephen Arthritis Liquid', \n",
    "    'Acephen Arthritis Chewable', 'Acephen Arthritis Caplets', 'Acephen Arthritis Softgels', \n",
    "    'Acephen Arthritis Coated', 'Acephen Arthritis Extended', 'Acephen Arthritis Dual', \n",
    "    'Acephen Arthritis Triple', 'Acephen Junior Relief', 'Acephen Junior Max', \n",
    "    'Acephen Junior Daytime', 'Acephen Junior Nighttime', 'Acephen Junior Liquid', \n",
    "    'Acephen Junior Chewable', 'Acephen Junior Caplets', 'Acephen Junior Softgels', \n",
    "    'Acephen Junior Coated', 'Acephen Junior Extended', 'Acephen Junior Dual', \n",
    "    'Acephen Junior Triple', 'Acephen Infant Relief', 'Acephen Infant Max', \n",
    "    'Acephen Infant Daytime', 'Acephen Infant Nighttime', 'Acephen Infant Liquid', \n",
    "    'Acephen Infant Drops', 'Acephen Infant Suspension', 'Acephen Infant Syrup', \n",
    "    'Acephen Pediatric Relief', 'Acephen Pediatric Max', 'Acephen Pediatric Daytime', \n",
    "    'Acephen Pediatric Nighttime', 'Acephen Pediatric Liquid', 'Acephen Pediatric Drops', \n",
    "    'Acephen Pediatric Suspension', 'Acephen Pediatric Syrup', 'Acephen Liquid Relief', \n",
    "    'Acephen Liquid Max', 'Acephen Liquid Daytime', 'Acephen Liquid Nighttime', \n",
    "    'Acephen Liquid Fast Acting', 'Acephen Liquid Rapid Release', 'Acephen Chewable Relief', \n",
    "    'Acephen Chewable Max', 'Acephen Chewable Daytime', 'Acephen Chewable Nighttime', \n",
    "    'Acephen Chewable Fast Acting', 'Acephen Chewable Rapid Release', 'Acephen Caplet Relief', \n",
    "    'Acephen Caplet Max', 'Acephen Caplet Daytime', 'Acephen Caplet Nighttime', \n",
    "    'Acephen Caplet Fast Acting', 'Acephen Caplet Rapid Release', 'Acephen Coated Relief', \n",
    "    'Acephen Coated Max', 'Acephen Coated Daytime', 'Acephen Coated Nighttime', \n",
    "    'Acephen Coated Fast Acting', 'Acephen Coated Rapid Release', 'Acephen Time Release Relief', \n",
    "    'Acephen Time Release Max', 'Acephen Time Release Daytime', 'Acephen Time Release Nighttime', \n",
    "    'Acephen Time Release Fast Acting', 'Acephen Time Release Rapid Release', 'Acephen Forte Relief', \n",
    "    'Acephen Forte Max', 'Acephen Forte Daytime', 'Acephen Forte Nighttime', \n",
    "    'Acephen Forte Fast Acting', 'Acephen Forte Rapid Release', 'Acephen Maximum Strength Relief', \n",
    "    'Acephen Maximum Strength Max', 'Acephen Maximum Strength Daytime', \n",
    "    'Acephen Maximum Strength Nighttime', 'Acephen Maximum Strength Fast Acting', \n",
    "    'Acephen Maximum Strength Rapid Release', 'Acephen Allergy Relief Max', \n",
    "    'Acephen Allergy Relief Daytime', 'Acephen Allergy Relief Nighttime', \n",
    "    'Acephen Allergy Relief Fast Acting', 'Acephen Allergy Relief Rapid Release', \n",
    "    'Acephen Nighttime Relief', 'Acephen Nighttime Max', 'Acephen Nighttime Fast Acting', \n",
    "    'Acephen Nighttime Rapid Release', 'Acephen Daytime Relief', 'Acephen Daytime Max', \n",
    "    'Acephen Daytime Fast Acting', 'Acephen Daytime Rapid Release', 'Acephen Sinus Relief Max', \n",
    "    'Acephen Sinus Relief Daytime', 'Acephen Sinus Relief Nighttime', \n",
    "    'Acephen Sinus Relief Fast Acting', 'Acephen Sinus Relief Rapid Release', \n",
    "    'Acephen Multi-Symptom Relief', 'Acephen Multi-Symptom Max', \n",
    "    'Acephen Multi-Symptom Daytime', 'Acephen Multi-Symptom Nighttime', \n",
    "    'Acephen Multi-Symptom Fast Acting', 'Acephen Multi-Symptom Rapid Release', \n",
    "    'Acephen Extra Strength Relief', 'Acephen Extra Strength Max', \n",
    "    'Acephen Extra Strength Daytime', 'Acephen Extra Strength Nighttime', \n",
    "    'Acephen Extra Strength Fast Acting', 'Acephen Extra Strength Rapid Release', \n",
    "    'Acephen Rapid Melt Relief', 'Acephen Rapid Melt Max', 'Acephen Rapid Melt Daytime', \n",
    "    'Acephen Rapid Melt Nighttime', 'Acephen Rapid Melt Fast Acting', \n",
    "    'Acephen Rapid Melt Rapid Release', 'Acephen Softgels Relief', 'Acephen Softgels Max', \n",
    "    'Acephen Softgels Daytime', 'Acephen Softgels Nighttime', 'Acephen Softgels Fast Acting', \n",
    "    'Acephen Softgels Rapid Release', 'Acephen Coated Tablets Relief', \n",
    "    'Acephen Coated Tablets Max', 'Acephen Coated Tablets Daytime', \n",
    "    'Acephen Coated Tablets Nighttime', 'Acephen Coated Tablets Fast Acting', \n",
    "    'Acephen Coated Tablets Rapid Release', 'Acephen Extended Release Relief', \n",
    "    'Acephen Extended Release Max', 'Acephen Extended Release Daytime', \n",
    "    'Acephen Extended Release Nighttime', 'Acephen Extended Release Fast Acting', \n",
    "    'Acephen Extended Release Rapid Release', 'Acephen Dual Action Relief', \n",
    "    'Acephen Dual Action Max', 'Acephen Dual Action Daytime', 'Acephen Dual Action Nighttime', \n",
    "    'Acephen Dual Action Fast Acting', 'Acephen Dual Action Rapid Release', \n",
    "    'Acephen Triple Action Relief', 'Acephen Triple Action Max', \n",
    "    'Acephen Triple Action Daytime', 'Acephen Triple Action Nighttime', \n",
    "    'Acephen Triple Action Fast Acting', 'Acephen Triple Action Rapid Release', \n",
    "    'Acephen Plus Cold Relief', 'Acephen Plus Cold Max', 'Acephen Plus Cold Daytime', \n",
    "    'Acephen Plus Cold Nighttime', 'Acephen Plus Cold Fast Acting', \n",
    "    'Acephen Plus Cold Rapid Release', 'Acephen Plus Flu Relief', 'Acephen Plus Flu Max', \n",
    "    'Acephen Plus Flu Daytime', 'Acephen Plus Flu Nighttime', 'Acephen Plus Flu Fast Acting', \n",
    "    'Acephen Plus Flu Rapid Release', 'Acephen Plus Sinus Relief', 'Acephen Plus Sinus Max', \n",
    "    'Acephen Plus Sinus Daytime', 'Acephen Plus Sinus Nighttime', \n",
    "    'Acephen Plus Sinus Fast Acting', 'Acephen Plus Sinus Rapid Release', \n",
    "    'Acephen Plus Allergy Relief', 'Acephen Plus Allergy Max', \n",
    "    'Acephen Plus Allergy Daytime', 'Acephen Plus Allergy Nighttime', \n",
    "    'Acephen Plus Allergy Fast Acting', 'Acephen Plus Allergy Rapid Release', \n",
    "    'Acephen Plus Headache Relief', 'Acephen Plus Headache Max', \n",
    "    'Acephen Plus Headache Daytime', 'Acephen Plus Headache Nighttime', \n",
    "    'Acephen Plus Headache Fast Acting', 'Acephen Plus Headache Rapid Release', \n",
    "    'Acephen Plus Migraine Relief', 'Acephen Plus Migraine Max', \n",
    "    'Acephen Plus Migraine Daytime', 'Acephen Plus Migraine Nighttime', \n",
    "    'Acephen Plus Migraine Fast Acting', 'Acephen Plus Migraine Rapid Release', \n",
    "    'Acephen Plus Pain Relief', 'Acephen Plus Pain Max', 'Acephen Plus Pain Daytime', \n",
    "    'Acephen Plus Pain Nighttime', 'Acephen Plus Pain Fast Acting', \n",
    "    'Acephen Plus Pain Rapid Release', 'Acephen Plus Fever Reducer', \n",
    "    'Acephen Plus Fever Max', 'Acephen Plus Fever Daytime', 'Acephen Plus Fever Nighttime', \n",
    "    'Acephen Plus Fever Fast Acting', 'Acephen Plus Fever Rapid Release'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "元素 Sertralin 重复次数: 2\n",
      "元素 Imipramin 重复次数: 2\n",
      "元素 Lamictin 重复次数: 2\n",
      "元素 Tegretin 重复次数: 2\n",
      "元素 Desyrelin 重复次数: 2\n",
      "元素 Epitolin 重复次数: 2\n",
      "元素 Pamelorin 重复次数: 2\n",
      "元素 Depakolin 重复次数: 2\n",
      "元素 Miconazolum 重复次数: 3\n",
      "元素 Phenoleum 重复次数: 2\n",
      "元素 Hydroxychloroquin 重复次数: 3\n",
      "元素 Malaronex 重复次数: 2\n",
      "元素 Plaquenilix 重复次数: 2\n",
      "元素 Lariamide 重复次数: 2\n",
      "元素 Pyrimethamide 重复次数: 2\n",
      "元素 Acephen Plus Pain Relief 重复次数: 2\n",
      "元素 Acephen Plus Fever Reducer 重复次数: 2\n",
      "[False, True, False, False, False]\n",
      "[True, True, True, True, True]\n",
      "Acetaminophen / caffeine\n",
      "Acetaminophen\n",
      "Aspirin\n",
      "Acetaminophen / diphenhydramine\n",
      "Bayer Aspirin\n",
      "Tylenol\n",
      "Vivarin\n",
      "Tylenol 8 Hour\n",
      "Acetaminophen / phenyltoloxamine\n",
      "Feverall\n",
      "Acetaminophen / aspirin\n",
      "Vicks Dayquil Cold & Flu Relief\n",
      "Alka-Seltzer Plus Cold Formula Sparkling Original Effervescent Tablets\n",
      "重复：13\n"
     ]
    }
   ],
   "source": [
    "def data_preprocess(train_name, train_condition, train_class, test_name, test_condition, test_class):\n",
    "    train_combine = []\n",
    "    test_combine = []\n",
    "\n",
    "    train_class_num = []\n",
    "    test_class_num = []\n",
    "\n",
    "    for drug_name, drug_class in zip(train_name, train_class):\n",
    "        if drug_name in train_combine:\n",
    "            continue\n",
    "        else:\n",
    "            train_combine.append(drug_name)\n",
    "            train_class_num.append(Class_to_Num[drug_class])\n",
    "    \n",
    "    for drug_name, drug_class in zip(test_name, test_class):\n",
    "        if drug_name in test_combine:\n",
    "            continue\n",
    "        else:\n",
    "            test_combine.append(drug_name)\n",
    "            test_class_num.append(Class_to_Num[drug_class])\n",
    "\n",
    "\n",
    "\n",
    "    return train_combine, train_class_num, test_combine, test_class_num\n",
    "\n",
    "\n",
    "train_combine, train_class_num, test_combine, test_class_num = data_preprocess(train_name, train_condition, train_class, test_name, test_condition, test_class)\n",
    "\n",
    "\n",
    "def check_unique(augmented_classes):\n",
    "    for lst in augmented_classes:\n",
    "        count = Counter(lst)  # 统计每个元素的出现次数\n",
    "        duplicates = {key: value for key, value in count.items() if value > 1}\n",
    "        for element, freq in duplicates.items():\n",
    "            print(f\"元素 {element} 重复次数: {freq}\")\n",
    "    return [len(lst) == len(set(lst)) for lst in augmented_classes]\n",
    "\n",
    "\n",
    "def clean(augmented_classes):\n",
    "    output = []\n",
    "    for lst in augmented_classes:\n",
    "        seen = set()  # 用于记录已遇到的元素\n",
    "        result = []\n",
    "        for item in lst:\n",
    "            if item not in seen:\n",
    "                result.append(item)\n",
    "                seen.add(item)\n",
    "        output.append(result)\n",
    "    return output\n",
    "\n",
    "\n",
    "def drug_name_filter(augmented_classes, classes):\n",
    "    repeat = 0\n",
    "    for augmented_class, class_ in zip(augmented_classes, classes):\n",
    "        for augmented in augmented_class:\n",
    "            if augmented in train_combine or augmented in test_combine:\n",
    "                print(augmented)\n",
    "                repeat = repeat + 1\n",
    "                continue\n",
    "            else:\n",
    "                train_combine.append(augmented)\n",
    "                train_class_num.append(class_)\n",
    "    print(f\"重复：{repeat}\")\n",
    "    \n",
    "\n",
    "\n",
    "print(check_unique([augmented_Mood_Stabilizers, augmented_Antibiotics, augmented_Antiseptics, augmented_Antimalarial, augmented_Antipiretics]))\n",
    "augmented_Mood_Stabilizers, augmented_Antibiotics, augmented_Antiseptics, augmented_Antimalarial, augmented_Antipiretics = clean([augmented_Mood_Stabilizers, augmented_Antibiotics, augmented_Antiseptics, augmented_Antimalarial, augmented_Antipiretics])\n",
    "print(check_unique([augmented_Mood_Stabilizers, augmented_Antibiotics, augmented_Antiseptics, augmented_Antimalarial, augmented_Antipiretics]))\n",
    "\n",
    "\n",
    "drug_name_filter([augmented_Mood_Stabilizers, augmented_Antibiotics, augmented_Antiseptics, augmented_Antimalarial, augmented_Antipiretics], [1, 2, 3, 4, 5])\n",
    "\n",
    "# count_unique_strings(train_class_num)\n",
    "# count_unique_strings(test_class_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "总共有 6 个唯一的字符串。\n",
      "每个字符串的出现次数如下:\n",
      "'0': 369 次\n",
      "'1': 304 次\n",
      "'2': 386 次\n",
      "'3': 438 次\n",
      "'4': 391 次\n",
      "'5': 329 次\n",
      "总共有 6 个唯一的字符串。\n",
      "每个字符串的出现次数如下:\n",
      "'1': 62 次\n",
      "'2': 141 次\n",
      "'3': 42 次\n",
      "'0': 264 次\n",
      "'4': 9 次\n",
      "'5': 11 次\n"
     ]
    }
   ],
   "source": [
    "count_unique_strings(train_class_num)\n",
    "count_unique_strings(test_class_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 2217\n",
      "test: 529\n",
      "train: [3, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 34, 34, 34, 34, 34, 34, 34, 34, 34, 35, 35, 35, 35, 35, 35, 35, 35, 36, 36, 36, 36, 36, 36, 36, 37, 37, 37, 38, 38, 38, 39, 39, 40, 41, 41, 41, 42, 43, 45, 46, 47, 47, 47, 47, 47, 48, 48, 48, 49, 51, 52, 59, 70]\n",
      "test: [4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 14, 14, 14, 14, 14, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 16, 16, 16, 16, 16, 17, 17, 17, 17, 17, 17, 17, 17, 18, 18, 18, 18, 19, 19, 19, 19, 20, 20, 20, 20, 20, 20, 20, 20, 20, 21, 21, 21, 21, 21, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 24, 24, 24, 24, 24, 24, 24, 25, 25, 25, 25, 25, 25, 25, 26, 26, 26, 26, 26, 27, 27, 27, 28, 28, 29, 29, 29, 29, 29, 29, 30, 30, 31, 31, 31, 31, 32, 32, 33, 34, 35, 37, 38, 39, 41, 41, 43, 46, 47, 47, 47, 48, 48, 49, 52, 59]\n",
      "max: 70\n"
     ]
    }
   ],
   "source": [
    "def check_length(train_combine, test_combine):\n",
    "    print(f\"train: {sorted(len(s) for s in train_combine)}\")\n",
    "    print(f\"test: {sorted(len(s) for s in test_combine)}\")\n",
    "    print(f\"max: {max(max(len(s) for s in train_combine), max(sorted(len(s) for s in test_combine)))}\")\n",
    "\n",
    "\n",
    "print(f\"train: {len(train_combine)}\")\n",
    "print(f\"test: {len(test_combine)}\")\n",
    "check_length(train_combine, test_combine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "unk_token_id = tokenizer.convert_tokens_to_ids('[UNK]')\n",
    "print(unk_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Anaconda\\envs\\DeepLearning\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2700: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def tokenize(tokenizer, train_reviews, test_reviews):\n",
    "    train_reviews_token = [tokenizer.encode_plus(\n",
    "    text,\n",
    "    truncation=True,\n",
    "    add_special_tokens=True,\n",
    "    max_length=50,            \n",
    "    pad_to_max_length=True,  \n",
    "    return_attention_mask=True,  \n",
    "    return_tensors='pt',      \n",
    "    ) for text in train_reviews]\n",
    "\n",
    "    test_reviews_token = [tokenizer.encode_plus(\n",
    "    text,\n",
    "    truncation=True,\n",
    "    add_special_tokens=True,\n",
    "    max_length=50,            \n",
    "    pad_to_max_length=True,  \n",
    "    return_attention_mask=True,  \n",
    "    return_tensors='pt',      \n",
    "    ) for text in test_reviews]\n",
    "\n",
    "    return train_reviews_token, test_reviews_token\n",
    "\n",
    "\n",
    "train_combines_token, test_combines_token = tokenize(tokenizer, train_combine, test_combine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train unknown: 0\n",
      "test unknown: 0\n"
     ]
    }
   ],
   "source": [
    "def check_unknown(train_combines_token, test_combines_token):\n",
    "\n",
    "    train_combines_token_input_ids = [_['input_ids'] for _ in train_combines_token]\n",
    "    test_combines_token_input_ids = [_['input_ids'] for _ in test_combines_token]\n",
    "\n",
    "    train_count = sum(1 for t in train_combines_token_input_ids if (t == 100).any())\n",
    "    test_count = sum(1 for t in test_combines_token_input_ids if (t == 100).any())\n",
    "    print(f\"train unknown: {train_count}\")\n",
    "    print(f\"test unknown: {test_count}\")\n",
    "    \n",
    "check_unknown(train_combines_token, test_combines_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Review_Rating_Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, reviews_token, class_):\n",
    "        self.review = reviews_token\n",
    "        self.class_ = class_\n",
    " \n",
    "    def __getitem__(self, idx):\n",
    "        item = {k: v.squeeze(dim=0) for k, v in self.review[idx].items()}\n",
    "        item[\"class\"] = torch.tensor(self.class_[idx])\n",
    "        return item\n",
    " \n",
    "    def __len__(self):\n",
    "        return len(self.class_)\n",
    "\n",
    "\n",
    "train_dataset = Review_Rating_Dataset(train_combines_token, train_class_num)\n",
    "test_dataset = Review_Rating_Dataset(test_combines_token, test_class_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertWithMLP(nn.Module):\n",
    "    def __init__(self, bert, hidden_size=768, mlp_hidden_size1=1024, mlp_hidden_size2 =256, num_classes=10):\n",
    "        super(BertWithMLP, self).__init__()\n",
    "        self.bert = bert\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(hidden_size, mlp_hidden_size2),\n",
    "            nn.ReLU(),\n",
    "            # nn.Dropout(0.2),\n",
    "            # nn.Linear(mlp_hidden_size1, mlp_hidden_size2),\n",
    "            # nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(mlp_hidden_size2, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        cls = outputs.last_hidden_state[:, 0, :]\n",
    "        \n",
    "        logits = self.mlp(cls)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, optimizer, scheduler, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    # total_error = 0.0\n",
    "    \n",
    "    progress_bar = tqdm(dataloader, desc=\"Training\", leave=False)\n",
    "    for batch in progress_bar:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['class'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        # loss = criterion(outputs, labels.float())\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        # preds = torch.round(outputs)\n",
    "        preds = torch.argmax(outputs, dim=-1)\n",
    "        correct_predictions += torch.sum(preds == labels)\n",
    "        # total_error += torch.sum(torch.abs(labels - outputs))\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # 更新进度条显示\n",
    "        progress_bar.set_postfix({\n",
    "            'loss': loss.item(),\n",
    "            'acc': torch.sum(preds == labels).item()/len(labels),\n",
    "            # 'error': torch.mean(torch.abs(labels - outputs)).item()\n",
    "        })\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = correct_predictions.double() / len(dataloader.dataset)\n",
    "    # error = total_error.item() / len(dataloader.dataset)\n",
    "    # return avg_loss, accuracy, error\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "\n",
    "def eval_model(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    # total_error = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        progress_bar = tqdm(dataloader, desc=\"Evaluating\", leave=False)\n",
    "        for batch in progress_bar:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['class'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            loss = criterion(outputs, labels)\n",
    "            # loss = criterion(outputs, labels.float())\n",
    "            \n",
    "            preds = torch.argmax(outputs, dim=-1)\n",
    "            correct_predictions += torch.sum(preds == labels)\n",
    "            # total_error += torch.sum(torch.abs(labels - outputs))\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            progress_bar.set_postfix({\n",
    "                'loss': loss.item(),\n",
    "                'acc': torch.sum(preds == labels).item()/len(labels),\n",
    "                # 'error': torch.mean(torch.abs(labels - outputs)).item()\n",
    "            })\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = correct_predictions.double() / len(dataloader.dataset)\n",
    "    # error = total_error.item() / len(dataloader.dataset)\n",
    "    # return avg_loss, accuracy, error\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "# 4. 主训练循环\n",
    "def train_and_evaluate(\n",
    "    model, \n",
    "    train_loader, \n",
    "    val_loader, \n",
    "    optimizer, \n",
    "    scheduler,\n",
    "    criterion, \n",
    "    device, \n",
    "    epochs, \n",
    "    model_save_path,\n",
    "    eval_every=1  # 每多少轮评估一次\n",
    "):\n",
    "    # best_val_error = 0.0\n",
    "    best_val_acc = 0.0\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'train_acc': [],\n",
    "        # 'train_error': [],\n",
    "        'val_loss': [],\n",
    "        'val_acc': [],\n",
    "        # 'val_error': []\n",
    "    }\n",
    "    \n",
    "    for epoch in range(1, epochs+1):\n",
    "        print(f\"\\nEpoch {epoch}/{epochs}\")\n",
    "        \n",
    "        # 训练阶段\n",
    "        # train_loss, train_acc, train_error = train_epoch(\n",
    "        #     model, train_loader, optimizer, criterion, device)\n",
    "        train_loss, train_acc = train_epoch(\n",
    "            model, train_loader, optimizer, scheduler, criterion, device)\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc.item())\n",
    "        # history['train_error'].append(train_error)\n",
    "        \n",
    "        # print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | Train Error: {train_error:.4f}\")\n",
    "        print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
    "        \n",
    "        # 验证阶段\n",
    "        if epoch % eval_every == 0 and val_loader is not None:\n",
    "            # val_loss, val_acc, val_error = eval_model(\n",
    "            #     model, val_loader, criterion, device)\n",
    "            val_loss, val_acc = eval_model(\n",
    "                model, val_loader, criterion, device)\n",
    "            history['val_loss'].append(val_loss)\n",
    "            history['val_acc'].append(val_acc.item())\n",
    "            # history['val_error'].append(val_error)\n",
    "            \n",
    "            # print(f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f} | Val Error: {val_error:.4f}\")\n",
    "            print(f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "            \n",
    "            # if val_error > best_val_error:\n",
    "            #     best_val_error = val_error\n",
    "            #     torch.save(model.state_dict(), model_save_path)\n",
    "            #     print(f\"New best model saved to {model_save_path} with val_acc: {val_acc:.4f} | val_error: {val_error:.4f}\")\n",
    "\n",
    "            #     continue\n",
    "\n",
    "            # 保存最佳模型\n",
    "            if val_acc > best_val_acc:\n",
    "                best_val_acc = val_acc\n",
    "                torch.save(model.state_dict(), model_save_path)\n",
    "                # print(f\"New best model saved to {model_save_path} with val_acc: {val_acc:.4f} | val_error: {val_error:.4f}\")\n",
    "                print(f\"New best model saved to {model_save_path} with val_acc: {val_acc:.4f}\")\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.6655 | Train Acc: 0.3054\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 1.6608 | Val Acc: 0.2628\n",
      "New best model saved to ./drug_class_prediction_best_model.pth with val_acc: 0.2628\n",
      "\n",
      "Epoch 2/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.0053 | Train Acc: 0.6351\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 1.1314 | Val Acc: 0.5180\n",
      "New best model saved to ./drug_class_prediction_best_model.pth with val_acc: 0.5180\n",
      "\n",
      "Epoch 3/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5166 | Train Acc: 0.8151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.4438 | Val Acc: 0.8582\n",
      "New best model saved to ./drug_class_prediction_best_model.pth with val_acc: 0.8582\n",
      "\n",
      "Epoch 4/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2500 | Train Acc: 0.9251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.1647 | Val Acc: 0.9471\n",
      "New best model saved to ./drug_class_prediction_best_model.pth with val_acc: 0.9471\n",
      "\n",
      "Epoch 5/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1167 | Train Acc: 0.9675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.0788 | Val Acc: 0.9754\n",
      "New best model saved to ./drug_class_prediction_best_model.pth with val_acc: 0.9754\n",
      "\n",
      "Epoch 6/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0545 | Train Acc: 0.9860\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.0413 | Val Acc: 0.9905\n",
      "New best model saved to ./drug_class_prediction_best_model.pth with val_acc: 0.9905\n",
      "\n",
      "Epoch 7/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0303 | Train Acc: 0.9919\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.0046 | Val Acc: 0.9981\n",
      "New best model saved to ./drug_class_prediction_best_model.pth with val_acc: 0.9981\n",
      "\n",
      "Epoch 8/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0172 | Train Acc: 0.9955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.0034 | Val Acc: 0.9981\n",
      "\n",
      "Epoch 9/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0038 | Train Acc: 0.9991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.0050 | Val Acc: 0.9962\n",
      "\n",
      "Epoch 10/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0105 | Train Acc: 0.9973\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.0001 | Val Acc: 1.0000\n",
      "New best model saved to ./drug_class_prediction_best_model.pth with val_acc: 1.0000\n",
      "\n",
      "Epoch 11/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0032 | Train Acc: 0.9991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.0061 | Val Acc: 0.9981\n",
      "\n",
      "Epoch 12/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0026 | Train Acc: 0.9986\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.0023 | Val Acc: 0.9981\n",
      "\n",
      "Epoch 13/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0014 | Train Acc: 0.9995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.0002 | Val Acc: 1.0000\n",
      "\n",
      "Epoch 14/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0010 | Train Acc: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.0001 | Val Acc: 1.0000\n",
      "\n",
      "Epoch 15/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0012 | Train Acc: 0.9995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.0001 | Val Acc: 1.0000\n",
      "\n",
      "Training complete!\n",
      "Best validation accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # 初始化\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    BERT = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "    model = BertWithMLP(BERT, hidden_size=768, mlp_hidden_size1=1024, mlp_hidden_size2=256, num_classes=6)\n",
    "    model.to(device)\n",
    "\n",
    "    # 参数分组\n",
    "    no_decay = ['bias', 'LayerNorm.weight']\n",
    "    bert_params = []\n",
    "    mlp_params = []\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'mlp' in name:  # MLP层参数\n",
    "            mlp_params.append((name, param))\n",
    "        else:  # BERT参数\n",
    "            bert_params.append((name, param))\n",
    "\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in bert_params if not any(nd in n for nd in no_decay)],\n",
    "        'weight_decay': 0.01,\n",
    "        'lr': 2e-5},  # BERT主体较小学习率\n",
    "        \n",
    "        {'params': [p for n, p in bert_params if any(nd in n for nd in no_decay)],\n",
    "        'weight_decay': 0.0,\n",
    "        'lr': 2e-5},\n",
    "        \n",
    "        {'params': [p for n, p in mlp_params if not any(nd in n for nd in no_decay)],\n",
    "        'weight_decay': 0.01,\n",
    "        'lr': 1e-4},  # MLP层较大学习率\n",
    "        \n",
    "        {'params': [p for n, p in mlp_params if any(nd in n for nd in no_decay)],\n",
    "        'weight_decay': 0.0,\n",
    "        'lr': 1e-4}\n",
    "    ]\n",
    "\n",
    "    optimizer = AdamW(optimizer_grouped_parameters)\n",
    "\n",
    "    epochs = 15\n",
    "\n",
    "    total_steps = len(train_loader) * epochs\n",
    "    warmup_steps = int(0.1 * total_steps)  # 10%的warmup\n",
    "\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=warmup_steps,\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    model_save_path = \"./drug_class_prediction_best_model.pth\"\n",
    "    \n",
    "    # 创建保存目录\n",
    "    os.makedirs(os.path.dirname(model_save_path), exist_ok=True)\n",
    "    \n",
    "    # 训练和验证\n",
    "    history = train_and_evaluate(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=test_loader,\n",
    "        optimizer=optimizer,\n",
    "        scheduler=scheduler,\n",
    "        criterion=criterion,\n",
    "        device=device,\n",
    "        epochs=epochs,\n",
    "        model_save_path=model_save_path,\n",
    "        eval_every=1  # 每轮都验证\n",
    "    )\n",
    "    \n",
    "    print(\"\\nTraining complete!\")\n",
    "    print(f\"Best validation accuracy: {max(history['val_acc']):.4f}\")\n",
    "    # print(f\"Best validation error: {max(history['val_error']):.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
