{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2eba775",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "find 3436 unique drug names in the ./data/drugsComTrain_raw.csv file.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import random\n",
    "\n",
    "CSV_PATH = \"./data/drugsComTrain_raw.csv\"\n",
    "DRUG_COLUMN = \"drugName\"\n",
    "\n",
    "# read drug names from CSV file\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "drug_names = df[DRUG_COLUMN].unique().tolist()\n",
    "print(f\"find {len(drug_names)} unique drug names in the {CSV_PATH} file.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52982f06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original drug name: Aspirin 81 mg (Enteric Coated) (Bayer)\n",
      "Reconstructed drug name: Aspirin_81_mg_Enteric_Coated_Bayer\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def reconstruct(drug_name):\n",
    "    \"\"\"\n",
    "    Reconstruct the drug name by removing special characters and spaces.\n",
    "    \"\"\"\n",
    "    # Remove special characters and spaces\n",
    "    drug_name = re.split(r'[^a-zA-Z0-9]', drug_name)\n",
    "    return '_'.join(filter(None, drug_name))\n",
    "\n",
    "# example usage\n",
    "drug_name = \"Aspirin 81 mg (Enteric Coated) (Bayer)\"\n",
    "reconstructed_name = reconstruct(drug_name)\n",
    "print(f\"Original drug name: {drug_name}\")\n",
    "print(f\"Reconstructed drug name: {reconstructed_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06fe0f08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching https://www.drugs.com/valsartan.html\n",
      "生成PDF时出错: wkhtmltopdf reported an error:\n",
      "Exit with code 1 due to network error: ProtocolUnknownError\n",
      "\n",
      "Fetching https://www.drugs.com/guanfacine.html\n",
      "生成PDF时出错: wkhtmltopdf reported an error:\n",
      "Exit with code 1 due to network error: ProtocolUnknownError\n",
      "\n",
      "Fetching https://www.drugs.com/lybrel.html\n"
     ]
    }
   ],
   "source": [
    "import pdfkit\n",
    "import os\n",
    "import tempfile\n",
    "\n",
    "path_to_wkhtmltopdf = r'C:\\Program Files\\wkhtmltopdf\\bin\\wkhtmltopdf.exe'\n",
    "config = pdfkit.configuration(wkhtmltopdf=path_to_wkhtmltopdf)\n",
    "\n",
    "output_dir = './test1/'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "def generate_pdf(url, output_name, output_dir):\n",
    "    \"\"\"\n",
    "    抓取药物网页信息，清洗内容，并生成PDF文件\n",
    "    \n",
    "    Args:\n",
    "        url (str): 药物信息页面的URL\n",
    "        output_name (str): 输出PDF文件名\n",
    "        output_dir (str): 输出目录\n",
    "        \n",
    "    Returns:\n",
    "        int: 成功返回1，失败返回None\n",
    "    \"\"\"\n",
    "    # 创建HTML子目录用于保存中间文件\n",
    "    html_dir = os.path.join(output_dir, 'html')\n",
    "    os.makedirs(html_dir, exist_ok=True)\n",
    "    \n",
    "    # 检查URL是否存在\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"URL {url} 不存在或无法访问 (状态码: {response.status_code})，跳过PDF生成\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"访问 {url} 时出错: {str(e)}，跳过PDF生成\")\n",
    "        return None\n",
    "    \n",
    "    # 解析HTML内容\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # 清洗内容\n",
    "    \n",
    "    # 1. 移除广告相关内容\n",
    "    for ad_div in soup.select('.display-ad, .display-ad-leaderboard, .ddc-content-promo-interactions'):\n",
    "        if ad_div:\n",
    "            ad_div.decompose()\n",
    "    \n",
    "    # 2. 移除Related/similar drugs部分\n",
    "    related_drugs_header = soup.find('h2', id='related-drugs')\n",
    "    if related_drugs_header:\n",
    "        # 移除标题及之后的轮播内容\n",
    "        carousel = related_drugs_header.find_next('div', class_='ddc-carousel-scroll')\n",
    "        if related_drugs_header:\n",
    "            related_drugs_header.decompose()\n",
    "        if carousel:\n",
    "            carousel.decompose()\n",
    "    \n",
    "    # 3. 移除More about药物部分及其后内容\n",
    "    more_resources = soup.find('div', id='more-resources')\n",
    "    if more_resources:\n",
    "        # 移除该div及之后的所有兄弟元素\n",
    "        current = more_resources\n",
    "        while current:\n",
    "            next_sibling = current.find_next_sibling()\n",
    "            current.decompose()\n",
    "            current = next_sibling\n",
    "    \n",
    "    # 4. 移除页脚\n",
    "    footer = soup.find('footer')\n",
    "    if footer:\n",
    "        footer.decompose()\n",
    "    \n",
    "    # 5. 移除头部导航栏\n",
    "    header = soup.find('header')\n",
    "    if header:\n",
    "        header.decompose()\n",
    "    \n",
    "    # 6. 移除侧边栏\n",
    "    sidebar = soup.find('div', id='sidebar')\n",
    "    if sidebar:\n",
    "        sidebar.decompose()\n",
    "    \n",
    "    # 7. 移除社交分享按钮\n",
    "    social_share = soup.find('div', class_='ddc-social-share')\n",
    "    if social_share:\n",
    "        social_share.decompose()\n",
    "    \n",
    "    # 8. 移除页面图标按钮\n",
    "    page_icons = soup.find('div', class_='page-icons')\n",
    "    if page_icons:\n",
    "        page_icons.decompose()\n",
    "        \n",
    "    # 9. 移除Play pronunciation按钮\n",
    "    pronunciation_button = soup.find('button', id='pronunciation')\n",
    "    if pronunciation_button:\n",
    "        pronunciation_button.decompose()\n",
    "        \n",
    "    # 10. 移除发音音频元素\n",
    "    audio_element = soup.find('audio', id='pronounce-audio')\n",
    "    if audio_element:\n",
    "        audio_element.decompose()\n",
    "        \n",
    "    # 保存原始HTML (可选，用于调试)\n",
    "    html_filename = output_name.replace('.pdf', '.html')\n",
    "    original_html_path = os.path.join(html_dir, f\"original_{html_filename}\")\n",
    "    with open(original_html_path, 'wb') as f:\n",
    "        f.write(response.content)\n",
    "    \n",
    "    # 保存清洗后的HTML\n",
    "    cleaned_html_path = os.path.join(html_dir, html_filename)\n",
    "    with open(cleaned_html_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(str(soup))\n",
    "    \n",
    "    try:\n",
    "        # 设置wkhtmltopdf配置\n",
    "        path_to_wkhtmltopdf = r'C:\\Program Files\\wkhtmltopdf\\bin\\wkhtmltopdf.exe'\n",
    "        config = pdfkit.configuration(wkhtmltopdf=path_to_wkhtmltopdf)\n",
    "        \n",
    "        # 设置PDF生成选项，提高稳定性\n",
    "        options = {\n",
    "            'quiet': '',\n",
    "            'encoding': 'UTF-8',\n",
    "            'no-outline': None\n",
    "        }\n",
    "        \n",
    "        # 从本地清洗后的HTML文件生成PDF\n",
    "        output_path = os.path.join(output_dir, output_name)\n",
    "        pdfkit.from_file(cleaned_html_path, output_path, configuration=config, options=options)\n",
    "        print(f\"PDF saved as {output_name}\")\n",
    "        \n",
    "        return 1\n",
    "    except Exception as e:\n",
    "        print(f\"生成PDF时出错: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# drug_names = drug_names[:10]  # 仅处理前10个药物名称以进行测试\n",
    "# 将找不到的药物名称存储在一个列表中\n",
    "not_found_drugs = []\n",
    "found_drugs = []\n",
    "for drug_name in drug_names:\n",
    "    # 生成药物信息页面的 URL\n",
    "    new_drug_name = drug_name.replace(\" \", \"-\").lower()\n",
    "    url = f\"https://www.drugs.com/{new_drug_name}.html\"\n",
    "    print(f\"Fetching {url}\")\n",
    "    # 将所有的非字母数字下划线的符号转为下划线\n",
    "    new_drug_name = reconstruct(drug_name)\n",
    "    output_name = f\"{new_drug_name}.pdf\"\n",
    "    result = generate_pdf(url, output_name,output_dir)\n",
    "    if not result:\n",
    "        not_found_drugs.append(drug_name)\n",
    "    else:\n",
    "        found_drugs.append(drug_name)\n",
    "print(not_found_drugs)\n",
    "\n",
    "print(f'found {len(found_drugs)} drugs, not found {len(not_found_drugs)} drugs.')\n",
    "# 将找不到的药物名称保存到 CSV 文件\n",
    "pd.DataFrame(not_found_drugs, columns=[\"not_found_1\"]).to_csv(os.path.join(output_dir, \"not_found_drugs.csv\"), index=False)\n",
    "# 将找到的药物名称保存到 CSV 文件\n",
    "pd.DataFrame(found_drugs, columns=[\"found_1\"]).to_csv(os.path.join(output_dir, \"found_drugs.csv\"), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c3dcad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "chrome_driver_path = \"C:\\\\Program Files\\\\Google\\\\Chrome\\\\Application\\\\chromedriver.exe\"\n",
    "# 设置Chrome选项\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")  # 无头模式，不显示浏览器窗口\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "chrome_options.add_argument(\"--enable-unsafe-swiftshader\")\n",
    "chrome_options.add_argument(\"--ignore-certificate-errors\")\n",
    "chrome_options.add_argument(\"--ignore-ssl-errors\")\n",
    "\n",
    "# 创建一个单独的WebDriver实例，避免多次创建和销毁\n",
    "driver = webdriver.Chrome(service=Service(executable_path=chrome_driver_path), options=chrome_options)\n",
    "\n",
    "def get_first_drug_result_url(drug_name):\n",
    "    \"\"\"\n",
    "    根据药品名称在drugs.com搜索并获取第一个结果的URL\n",
    "    \"\"\"\n",
    "    try:\n",
    "        search_url = f\"https://www.drugs.com/search.php?searchterm={drug_name}\"\n",
    "        print(f\"正在搜索: {drug_name} - {search_url}\")\n",
    "        driver.get(search_url)\n",
    "        \n",
    "        # 基于实际HTML结构更新选择器列表\n",
    "        selectors = [\n",
    "            \"a.ddc-search-result-link-wrap\",  # 主要目标：直接链接包装元素\n",
    "        ]\n",
    "        \n",
    "        result_url = None\n",
    "        for selector in selectors:\n",
    "            try:\n",
    "                # 短暂等待元素加载\n",
    "                WebDriverWait(driver, 3).until(\n",
    "                    EC.presence_of_element_located((By.CSS_SELECTOR, selector))\n",
    "                )\n",
    "                elements = driver.find_elements(By.CSS_SELECTOR, selector)\n",
    "                if elements:\n",
    "                    for element in elements:\n",
    "                        url = element.get_attribute(\"href\")\n",
    "                    # 如果没找到精确匹配，返回第一个结果\n",
    "                    result_url = elements[0].get_attribute(\"href\")\n",
    "                    print(f\"使用选择器 '{selector}' 找到结果: {result_url}\")\n",
    "                    break\n",
    "            except Exception as e:\n",
    "                print(f\"选择器 '{selector}' 查找失败\")\n",
    "        \n",
    "        if not result_url:\n",
    "            print(f\"未找到 {drug_name} 的搜索结果\")\n",
    "            return None\n",
    "            \n",
    "        return result_url\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"搜索 {drug_name} 时发生错误\")\n",
    "        return None\n",
    "\n",
    "not_found_drugs = pd.read_csv(os.path.join(output_dir, \"not_found_drugs.csv\"))[\"not_found_1\"].tolist()\n",
    "output_dir = './test2/'\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "not_found_drugs_2 = []\n",
    "found_drugs_2 = []\n",
    "for drug_name in not_found_drugs:\n",
    "    result_url = get_first_drug_result_url(drug_name)\n",
    "\n",
    "    if result_url:\n",
    "        print(f\"最终获取的URL是: {result_url}\")\n",
    "        new_drug_name = reconstruct(drug_name)\n",
    "        output_name = f\"{new_drug_name}.pdf\"\n",
    "        generate_pdf(result_url, output_name,output_dir)\n",
    "        found_drugs_2.append(drug_name)\n",
    "    else:\n",
    "        print(f\"未能获取 {drug_name} 的URL\")\n",
    "        not_found_drugs_2.append(drug_name)\n",
    "\n",
    "driver.quit()\n",
    "print(f'found {len(found_drugs_2)} drugs, not found {len(not_found_drugs_2)} drugs.')\n",
    "pd.DataFrame(not_found_drugs_2, columns=[\"not_found_2\"]).to_csv(os.path.join(output_dir, \"not_found_drugs_2.csv\"), index=False)\n",
    "pd.DataFrame(found_drugs_2, columns=[\"found_2\"]).to_csv(os.path.join(output_dir, \"found_drugs_2.csv\"), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_7008",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
