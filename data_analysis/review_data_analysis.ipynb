{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T11:56:50.897702Z",
     "start_time": "2025-04-24T11:56:49.565988Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from collections import Counter\n",
    "from itertools import combinations\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from textblob import TextBlob\n",
    "import joblib\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score"
   ],
   "id": "66040d241f844de9",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T11:59:56.978307Z",
     "start_time": "2025-04-24T11:59:56.926307Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ReviewDataAnalysis:\n",
    "    def __init__(self, data_path):\n",
    "        self.data_path = data_path\n",
    "        self.data = pd.read_csv(data_path)\n",
    "        self.data['date'] = pd.to_datetime(self.data['date'], format='%d-%b-%y')\n",
    "\n",
    "\n",
    "    def calculate_numeric_stats(self):\n",
    "        \"\"\"\n",
    "        Numeric data distribution analysis\n",
    "        \"\"\"\n",
    "        df = self.data.copy()\n",
    "        numeric_cols = ['rating', 'usefulCount']\n",
    "        result = {}\n",
    "        \n",
    "        for col in numeric_cols:\n",
    "            stats = {\n",
    "                'mean': round(df[col].mean(), 2),\n",
    "                'median': df[col].median(),\n",
    "                'std': round(df[col].std(), 2),\n",
    "                'min': df[col].min(),\n",
    "                'max': df[col].max(),\n",
    "                'skewness': round(df[col].skew(), 2),\n",
    "                'outliers': len(df[df[col] > df[col].quantile(0.99)])\n",
    "            }\n",
    "            result[col] = stats\n",
    "        \n",
    "        return {'numeric_distribution': result}\n",
    "\n",
    "\n",
    "    def analyze_categorical_data(self):\n",
    "        \"\"\"\n",
    "        Categorical data distribution analysis\n",
    "        \"\"\"\n",
    "        df = self.data.copy()\n",
    "        cat_cols = ['drugName', 'condition', 'Product Class']\n",
    "        result = {}\n",
    "        \n",
    "        for col in cat_cols:\n",
    "            if col == 'Product Class':\n",
    "                if 'Else' in df[col].unique():\n",
    "                    unique_values = df[col].nunique() - 1\n",
    "                else:\n",
    "                    unique_values = df[col].nunique()\n",
    "            else:\n",
    "                unique_values = df[col].nunique()\n",
    "            stats = {'unique_count': unique_values}\n",
    "            \n",
    "            if unique_values < 10:\n",
    "                if col == 'Product Class':\n",
    "                    # ignore 'Else' category\n",
    "                    filtered_counts = df[df[col] != 'Else'][col].value_counts().to_dict()\n",
    "                    stats['all_values_except_else'] = filtered_counts\n",
    "                else:\n",
    "                    stats['all_values'] = df[col].value_counts().to_dict()\n",
    "            else:\n",
    "                if col == 'Product Class':\n",
    "                    # ignore 'Else' category\n",
    "                    filtered_counts = df[df[col] != 'Else'][col].value_counts().head(5).to_dict()\n",
    "                    stats['top_5_except_else'] = filtered_counts\n",
    "                else:\n",
    "                    stats['top_5'] = df[col].value_counts().head(5).to_dict()\n",
    "            \n",
    "            result[col] = stats\n",
    "        \n",
    "        return {'categorical_distribution': result}\n",
    "    \n",
    "    def process_time_series(self):\n",
    "        \"\"\"\n",
    "        Time distribution\n",
    "        \"\"\"\n",
    "        df = self.data.copy()\n",
    "        result = {\n",
    "            'time_span': f\"{df['date'].min().date()} ~ {df['date'].max().date()}\",\n",
    "            'records_by_year': df['date'].dt.year.value_counts().to_dict(),\n",
    "            'records_by_month': df['date'].dt.month_name().value_counts().to_dict()\n",
    "        }\n",
    "        return {'temporal_distribution': result}\n",
    "    \n",
    "    \n",
    "    def _calculate_drug_rating_stats(self):\n",
    "        \"\"\"\n",
    "        Base drug rating statistics\n",
    "        \"\"\"\n",
    "        df = self.data.copy()\n",
    "        grouped = df.groupby('drugName')['rating']\n",
    "        stats_df = grouped.agg(['mean', 'count', 'median']).reset_index()\n",
    "        stats_df = stats_df.rename(columns={'mean': 'avg_rating', 'count': 'review_count', 'median': 'median_rating'})\n",
    "        \n",
    "        top_10_drugs = stats_df.sort_values(by='review_count', ascending=False).head(10)\n",
    "        \n",
    "        result = {\n",
    "            'global_summary': {\n",
    "                'highest_avg': round(stats_df['avg_rating'].max(), 2),\n",
    "                'lowest_avg': round(stats_df['avg_rating'].min(), 2),\n",
    "                'total_drugs': len(stats_df),\n",
    "                'total_reviews': int(stats_df['review_count'].sum())\n",
    "            },\n",
    "            'drug_rating_count_top10': top_10_drugs.set_index('drugName').to_dict('index')\n",
    "        }\n",
    "        return {'drug_rating_stats': result}\n",
    "    \n",
    "    def _analyze_usefulcount_correlation(self):\n",
    "        \"\"\"\n",
    "        UsefulCount-rating correlation analysis\n",
    "        :return: \n",
    "        \"\"\"\n",
    "        df = self.data.copy()\n",
    "        corr_value = round(df['rating'].corr(df['usefulCount']), 3)\n",
    "        \n",
    "        bins = [0, 5, 7, 10]\n",
    "        labels = ['0-5', '5-7', '7-10']\n",
    "        \n",
    "        df['rating_range'] = pd.cut(df['rating'], bins, labels=labels)\n",
    "        grouped = df.groupby('rating_range', observed=True)['usefulCount']\n",
    "        \n",
    "        segment_stats = grouped.agg(['mean', 'count']).rename(\n",
    "            columns={'mean': 'avg_usefulCount', 'count': 'record_count'}\n",
    "        )\n",
    "        \n",
    "        segment_dict = {}\n",
    "        for idx, row in segment_stats.iterrows():\n",
    "            segment_dict[str(idx)] = {\n",
    "                'avg_usefulCount': float(row['avg_usefulCount']),\n",
    "                'record_count': int(row['record_count'])\n",
    "            }\n",
    "        \n",
    "        result = {\n",
    "            'correlation_coefficient': corr_value,\n",
    "            'rating_segment_analysis': segment_dict,\n",
    "            'extreme_cases': {\n",
    "                'high_rating_low_useful': df[(df['rating'] >= 8) & (df['usefulCount'] <= 1)].shape[0],\n",
    "                'low_rating_high_useful': df[(df['rating'] <= 3) & (df['usefulCount'] >= 50)].shape[0]\n",
    "            }\n",
    "        }\n",
    "        return {'usefulcount_analysis': result}\n",
    "    \n",
    "    def _identify_extreme_rating_drugs(self, n=5):\n",
    "        \"\"\"\n",
    "        Extreme rating drugs analysis\n",
    "        \"\"\"\n",
    "        df = self.data.copy()\n",
    "        avg_ratings = df.groupby('drugName')['rating'].mean()\n",
    "        \n",
    "        result = {\n",
    "            'top_high_rating': avg_ratings.nlargest(n).to_dict(),\n",
    "            'top_low_rating': avg_ratings.nsmallest(n).to_dict(),\n",
    "            'controversial_drugs': df.groupby('drugName')['rating'].std()\n",
    "                                      .nlargest(n).to_dict()  # Drugs with the largest standard deviation in ratings\n",
    "        }\n",
    "        return {'extreme_rating_drugs': result}\n",
    "    \n",
    "    def perform_drug_rating_analysis(self):\n",
    "        \"\"\"\n",
    "        Drug-rating analysis \n",
    "        \"\"\"\n",
    "        result = {}\n",
    "        result.update(self._calculate_drug_rating_stats())\n",
    "        result.update(self._analyze_usefulcount_correlation())\n",
    "        result.update(self._identify_extreme_rating_drugs())\n",
    "        return {'drug_rating_analysis': result}\n",
    "    \n",
    "\n",
    "    def _calculate_condition_frequency(self):\n",
    "        \"\"\"\n",
    "        High frequency condition analysis \n",
    "        \"\"\"\n",
    "        df = self.data.copy()\n",
    "        df_clean = df.dropna(subset=['condition'])\n",
    "        condition_counts = df_clean['condition'].value_counts()\n",
    "        \n",
    "        result = {}\n",
    "        for condition, count in condition_counts.head(10).items():\n",
    "            sub_df = df_clean[df_clean['condition'] == condition]\n",
    "            result[condition] = {\n",
    "                'total_reviews': int(count),\n",
    "                'avg_rating': round(sub_df['rating'].mean(), 2),\n",
    "                'related_drugs_count': sub_df['drugName'].nunique(),\n",
    "                'top_drugs': sub_df['drugName'].value_counts().head(3).to_dict()\n",
    "            }\n",
    "        return {'condition_frequency': result}\n",
    "    \n",
    "    def _analyze_condition_associations(self):\n",
    "        \"\"\"\n",
    "        Condition associations analysis\n",
    "        \"\"\"\n",
    "        df = self.data.copy()\n",
    "        df_clean = df.dropna(subset=['condition'])\n",
    "        drug_conditions = df_clean.groupby('drugName')['condition'].unique()\n",
    "        \n",
    "        # co-occurrence analysis\n",
    "        co_occurrence = Counter()\n",
    "        for conditions in drug_conditions:\n",
    "            for pair in combinations(sorted(conditions), 2):\n",
    "                co_occurrence[pair] += 1\n",
    "        \n",
    "        # top 15 pairs\n",
    "        top_pairs = dict(co_occurrence.most_common(15))\n",
    "        return {'condition_associations': {\n",
    "            'most_common_pairs': {f\"{k[0]} & {k[1]}\": v for k, v in top_pairs.items()},\n",
    "            'multi_condition_drugs': sum(len(c) > 1 for c in drug_conditions)\n",
    "        }}\n",
    "    \n",
    "    def _identify_condition_rating_extremes(self):\n",
    "        \"\"\"\n",
    "        Condition rating extremes analysis\n",
    "        \"\"\"\n",
    "        df = self.data.copy()\n",
    "        df_clean = df.dropna(subset=['condition'])\n",
    "        grouped = df_clean.groupby('condition')['rating'].agg(['mean', 'count', 'std'])\n",
    "        \n",
    "        return {\n",
    "            'condition_rating_extremes': {\n",
    "                'highest_rated': grouped[grouped['count'] >= 10].nlargest(5, 'mean').to_dict('index'),\n",
    "                'lowest_rated': grouped[grouped['count'] >= 10].nsmallest(5, 'mean').to_dict('index'),\n",
    "                'most_controversial': grouped[grouped['count'] >= 10].nlargest(5, 'std').to_dict('index')\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def perform_condition_analysis(self):\n",
    "        \"\"\"\n",
    "        Condition analysis \n",
    "        \"\"\"\n",
    "        result = {}\n",
    "        result.update(self._calculate_condition_frequency())\n",
    "        result.update(self._analyze_condition_associations())\n",
    "        result.update(self._identify_condition_rating_extremes())\n",
    "        return {'condition_analysis': result}\n",
    "    \n",
    "    \n",
    "    def _preprocess_rating_data(self):\n",
    "        \"\"\"\n",
    "        Preprocess the rating data\n",
    "        \"\"\"\n",
    "        df = self.data.copy()\n",
    "        df_clean = df.copy()\n",
    "        # filter outliers\n",
    "        df_clean = df_clean.dropna(subset=['condition', 'Product Class', 'rating'])\n",
    "        # filter Else\n",
    "        df_clean = df_clean[df_clean['Product Class'] != 'Else']\n",
    "        return df_clean\n",
    "    \n",
    "    def _extract_temporal_features(self, df):\n",
    "        \"\"\"\n",
    "        Time series feature extraction\n",
    "        \"\"\"\n",
    "        df_temp = df.copy()\n",
    "        df_temp['year'] = pd.to_datetime(df['date']).dt.year\n",
    "        df_temp['month'] = pd.to_datetime(df['date']).dt.month\n",
    "        return df_temp[['year', 'month']]\n",
    "    \n",
    "    def _calculate_sentiment_scores(self, df):\n",
    "        \"\"\"\n",
    "        Sentiment score calculation using TextBlob\n",
    "        \"\"\"\n",
    "        df_temp = df.copy()\n",
    "        df_temp['sentiment'] = df['review'].apply(\n",
    "            lambda x: TextBlob(str(x)).sentiment.polarity\n",
    "        )\n",
    "        return df_temp[['sentiment']]\n",
    "    \n",
    "    def _build_feature_matrix(self, df):\n",
    "        \"\"\"\n",
    "        construct feature matrix for rating prediction\n",
    "        :param df: preprocessed DataFrame\n",
    "        :return: feature matrix\n",
    "        \"\"\"\n",
    "        # base features\n",
    "        base_features = pd.DataFrame({\n",
    "            'usefulCount': df['usefulCount'],\n",
    "            'review_length': df['review'].str.len()\n",
    "        }, index=df.index)\n",
    "        \n",
    "        # encode drugName and condition\n",
    "        encoder_drug = OneHotEncoder(sparse_output=False)\n",
    "        encoded_drug = pd.DataFrame(\n",
    "            encoder_drug.fit_transform(df[['drugName']]),\n",
    "            columns=[f\"drug_{x}\" for x in encoder_drug.categories_[0]],\n",
    "            index=df.index\n",
    "        )\n",
    "        encoder_condition = OneHotEncoder(sparse_output=False)\n",
    "        encoded_condition = pd.DataFrame(\n",
    "            encoder_condition.fit_transform(df[['condition']]),\n",
    "            columns=[f\"cond_{x}\" for x in encoder_condition.categories_[0]],\n",
    "            index=df.index\n",
    "        )\n",
    "        \n",
    "        feature_matrix = pd.concat([\n",
    "            base_features,\n",
    "            self._extract_temporal_features(df),\n",
    "            self._calculate_sentiment_scores(df),\n",
    "            encoded_drug.iloc[:, :10],  # top 10 frequent drugs\n",
    "            encoded_condition.iloc[:, :5]  # top 5 frequent conditions\n",
    "        ], axis=1)\n",
    "        \n",
    "        # merge all features\n",
    "        return feature_matrix\n",
    "    \n",
    "    \n",
    "    def _train_rating_prediction_model(self, X, y):\n",
    "        \"\"\"\n",
    "        Train a Random Forest model for rating prediction\n",
    "        \"\"\"\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "        model = RandomForestRegressor(n_estimators=100, max_depth=20, random_state=42)\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # save the model\n",
    "        model_dir = 'analysis_result/Review-data'\n",
    "        joblib.dump(model, f'{model_dir}/rating_prediction_model.pkl')\n",
    "        with open(f'{model_dir}/feature_columns.json', 'w') as f:\n",
    "            json.dump({'feature_columns': list(X.columns)}, f)\n",
    "        \n",
    "        preds = model.predict(X_test)\n",
    "        return {\n",
    "            'model_object': model,\n",
    "            'performance': {\n",
    "                'r2_score': round(r2_score(y_test, preds), 3),\n",
    "                'mse': round(mean_squared_error(y_test, preds), 2)\n",
    "            },\n",
    "            'feature_importance': dict(zip(\n",
    "                X.columns, \n",
    "                model.feature_importances_.round(3)\n",
    "            ))\n",
    "        }\n",
    "    \n",
    "    def perform_rating_prediction_analysis(self):\n",
    "        \"\"\"\n",
    "        Rating prediction analysis\n",
    "        \"\"\"\n",
    "        df = self.data.copy()\n",
    "        df_clean = self._preprocess_rating_data()\n",
    "        X = self._build_feature_matrix(df_clean)\n",
    "        y = df_clean['rating']\n",
    "        \n",
    "        # filter features\n",
    "        X_filtered = X.loc[:, X.columns.isin([\n",
    "            'usefulCount', 'review_length', 'year', 'month', 'sentiment'\n",
    "        ])]\n",
    "        model_results = self._train_rating_prediction_model(X_filtered, y)\n",
    "        return {'rating_prediction': {\n",
    "            'data_stats': {\n",
    "                'original_samples': len(df),\n",
    "                'cleaned_samples': len(df_clean),\n",
    "                'feature_dimension': X_filtered.shape[1]\n",
    "            },\n",
    "            'model_performance': model_results['performance'],\n",
    "            'top_features': dict(sorted(\n",
    "                model_results['feature_importance'].items(),\n",
    "                key=lambda x: x[1], \n",
    "                reverse=True\n",
    "            )[:])\n",
    "        }}\n",
    "    \n",
    "    \n",
    "    def _preprocess_clustering_data(self):\n",
    "        \"\"\"\n",
    "        Preprocess data for clustering\n",
    "        \"\"\"\n",
    "        df = self.data.copy()\n",
    "        df_clean = df.dropna(subset=['condition'])\n",
    "        df_clean = df_clean[df_clean['Product Class'] != 'Else']\n",
    "        return df_clean\n",
    "    \n",
    "    def _generate_cluster_features(self, df):\n",
    "        \"\"\"\n",
    "        Generate features for clustering\n",
    "        \"\"\"\n",
    "        drug_stats = df.groupby('drugName').agg(\n",
    "            avg_rating=('rating', 'mean'),\n",
    "            review_count=('uniqueID', 'count')\n",
    "        ).reset_index()\n",
    "        \n",
    "        top_conditions = df['condition'].value_counts().head(5).index.tolist()\n",
    "        condition_dummy = pd.get_dummies(df['condition']).groupby(df['drugName']).max()\n",
    "        condition_dummy = condition_dummy[top_conditions].add_prefix('cond_')\n",
    "        \n",
    "        feature_df = pd.merge(drug_stats, condition_dummy, on='drugName', how='left')\n",
    "        feature_df.fillna(0, inplace=True)\n",
    "        return feature_df.drop(columns='drugName'), feature_df['drugName']\n",
    "    \n",
    "    def _apply_kmeans_clustering(self, feature_matrix, n_clusters=3):\n",
    "        \"\"\"\n",
    "        Apply KMeans clustering and calculate silhouette score\n",
    "        :param feature_matrix: \n",
    "        :param n_clusters: \n",
    "        \"\"\"\n",
    "        scaler = StandardScaler()\n",
    "        scaled_features = scaler.fit_transform(feature_matrix)\n",
    "        kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "        cluster_labels = kmeans.fit_predict(scaled_features)\n",
    "        silhouette = silhouette_score(scaled_features, cluster_labels)  # 计算轮廓系数\n",
    "        return cluster_labels, silhouette  # 同时返回系数\n",
    "    \n",
    "    def _compile_cluster_analysis(self, feature_df, drug_names, cluster_labels):\n",
    "        \"\"\"\n",
    "        Compile cluster analysis results\n",
    "        :param feature_df: \n",
    "        :param drug_names:\n",
    "        :param cluster_labels:\n",
    "        \"\"\"\n",
    "        feature_df['cluster'] = cluster_labels\n",
    "        feature_df['drugName'] = drug_names\n",
    "        \n",
    "        result = {}\n",
    "        for cluster_id in feature_df['cluster'].unique():\n",
    "            cluster_data = feature_df[feature_df['cluster'] == cluster_id]\n",
    "            stats = {\n",
    "                'num_drugs': len(cluster_data),\n",
    "                'avg_rating': round(cluster_data['avg_rating'].mean(), 2),\n",
    "                'avg_review_count': int(cluster_data['review_count'].mean()),\n",
    "                'top_conditions': [\n",
    "                    col.replace('cond_', '') \n",
    "                    for col in cluster_data.filter(like='cond_').sum().sort_values(ascending=False).head(5).index\n",
    "                ]\n",
    "            }\n",
    "            result[f'cluster_{cluster_id}'] = stats\n",
    "        return result\n",
    "    \n",
    "    def analyze_drug_clusters(self):\n",
    "        \"\"\"\n",
    "        Drug clustering analysis\n",
    "        \"\"\"\n",
    "        df_clean = self._preprocess_clustering_data()\n",
    "        feature_matrix, drug_names = self._generate_cluster_features(df_clean)\n",
    "        cluster_labels, silhouette = self._apply_kmeans_clustering(feature_matrix)\n",
    "        # merge cluster labels with feature matrix\n",
    "        result = self._compile_cluster_analysis(feature_matrix.copy(), drug_names, cluster_labels)\n",
    "        result['silhouette_score'] = round(silhouette, 2)\n",
    "        \n",
    "        return {'drug_clustering': result}\n",
    "    \n",
    "    def analyze_all(self):\n",
    "        result = {}\n",
    "        result.update(self.calculate_numeric_stats())\n",
    "        result.update(self.analyze_categorical_data())\n",
    "        result.update(self.process_time_series())\n",
    "        result.update(self.perform_drug_rating_analysis())\n",
    "        result.update(self.perform_condition_analysis())\n",
    "        result.update(self.perform_rating_prediction_analysis())\n",
    "        result.update(self.analyze_drug_clusters())\n",
    "        \n",
    "        return result\n",
    "\n"
   ],
   "id": "290f843d60fb8ffd",
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-24T12:00:22.754316Z",
     "start_time": "2025-04-24T11:59:59.024623Z"
    }
   },
   "source": [
    "review_analysis = ReviewDataAnalysis('../data/drugsComTrain_raw_addclass.csv')\n",
    "review_analysis_result = review_analysis.analyze_all()\n",
    "\n",
    "if not os.path.exists('analysis_result/Review-data'):\n",
    "    os.makedirs('analysis_result/Review-data')\n",
    "\n",
    "with open(f'analysis_result/Review-data/review_analysis_result.json', 'w') as f:\n",
    "    json.dump(review_analysis_result, f, indent=4)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\anaconda3\\envs\\arin7102\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=3.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 3
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
