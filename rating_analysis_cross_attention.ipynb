{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from transformers import BertTokenizer, BertModel, get_linear_schedule_with_warmup\n",
    "from torch.optim import AdamW\n",
    "import torch\n",
    "from torch import nn\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv(train_path, test_path):\n",
    "    train_df = pd.read_csv(train_path)\n",
    "    test_df = pd.read_csv(test_path)\n",
    "\n",
    "    train_reviews = train_df[train_df['Product Class'] != 'Else']['review'].tolist()\n",
    "    train_ratings = train_df[train_df['Product Class'] != 'Else']['rating'].tolist()\n",
    "    train_usefulcount = train_df[train_df['Product Class'] != 'Else']['usefulCount'].tolist()\n",
    "\n",
    "    test_reviews = test_df[test_df['Product Class'] != 'Else']['review'].tolist()\n",
    "    test_ratings = test_df[test_df['Product Class'] != 'Else']['rating'].tolist()\n",
    "    test_usefulcount = test_df[test_df['Product Class'] != 'Else']['usefulCount'].tolist()\n",
    "\n",
    "    return train_reviews,train_ratings, train_usefulcount, test_reviews, test_ratings, test_usefulcount\n",
    "\n",
    "train_reviews, train_ratings, train_usefulcount, test_reviews, test_ratings, test_usefulcount = load_csv('./data/drugsComTrain_raw_addclass.csv', './data/drugsComTest_raw_addclass.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(torch.unique(torch.tensor(train_usefulcount)))\n",
    "# print(train_usefulcount)\n",
    "\n",
    "# train_usefulcount = torch.tensor(train_usefulcount).float()\n",
    "# print(((train_usefulcount - train_usefulcount.mean(dim=0, keepdim=True)) / train_usefulcount.std(dim=0, keepdim=True))[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Anaconda\\envs\\DeepLearning\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2700: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def tokenize(tokenizer, train_reviews, test_reviews):\n",
    "    train_reviews_token = [tokenizer.encode_plus(\n",
    "    text,\n",
    "    truncation=True,\n",
    "    add_special_tokens=True,\n",
    "    max_length=512,            \n",
    "    pad_to_max_length=True,  \n",
    "    return_attention_mask=True,  \n",
    "    return_tensors='pt',      \n",
    "    ) for text in train_reviews]\n",
    "\n",
    "    test_reviews_token = [tokenizer.encode_plus(\n",
    "    text,\n",
    "    truncation=True,\n",
    "    add_special_tokens=True,\n",
    "    max_length=512,            \n",
    "    pad_to_max_length=True,  \n",
    "    return_attention_mask=True,  \n",
    "    return_tensors='pt',      \n",
    "    ) for text in test_reviews]\n",
    "\n",
    "    return train_reviews_token, test_reviews_token\n",
    "\n",
    "\n",
    "train_reviews_token, test_reviews_token = tokenize(tokenizer, train_reviews, test_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Review_Rating_Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, reviews_token, rating, usefulcount):\n",
    "        self.review = reviews_token\n",
    "        self.rating = rating\n",
    "        self.usefulcount = torch.tensor(usefulcount)\n",
    " \n",
    "    def __getitem__(self, idx):\n",
    "        item = {k: v.squeeze(dim=0) for k, v in self.review[idx].items()}\n",
    "        item[\"rating\"] = torch.tensor(self.rating[idx] - 1)\n",
    "        item['usefulcount'] = self.usefulcount[idx]\n",
    "        return item\n",
    " \n",
    "    def __len__(self):\n",
    "        return len(self.rating)\n",
    "\n",
    "\n",
    "train_dataset = Review_Rating_Dataset(train_reviews_token, train_ratings, train_usefulcount)\n",
    "test_dataset = Review_Rating_Dataset(test_reviews_token, test_ratings, test_usefulcount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    \"Construct a layernorm module (See citation for details).\"\n",
    "\n",
    "    def __init__(self, features, eps=1e-6):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.a_2 = nn.Parameter(torch.ones(features))\n",
    "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2\n",
    "\n",
    "\n",
    "class CrossAttention(nn.Module):\n",
    "    def __init__(self, num_count, dim_count, dim_bert, hidden_size, num_head):\n",
    "        super(CrossAttention, self).__init__()\n",
    "        self.num_head = num_head\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.count_embeding = nn.Embedding(num_count, dim_count)\n",
    "\n",
    "        self.proj_Q = nn.Linear(dim_count, hidden_size)\n",
    "        self.proj_K = nn.Linear(dim_bert, hidden_size)\n",
    "        self.proj_V = nn.Linear(dim_bert, hidden_size)\n",
    "\n",
    "        self.linear = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "        self.attention_drop = nn.Dropout(0.1)\n",
    "        self.linear_drop = nn.Dropout(0.1)\n",
    "\n",
    "        self.layer_norm1 = LayerNorm(dim_count)\n",
    "        self.layer_norm2 = LayerNorm(hidden_size)\n",
    "        self.layer_norm3 = LayerNorm(hidden_size)\n",
    "\n",
    "        self.fnn1 = nn.Linear(hidden_size, 3072)\n",
    "        self.fnn2 = nn.Linear(3072, hidden_size)\n",
    "        self.fnn_dropout = nn.Dropout(0.1)\n",
    "    \n",
    "    def forward(self, bert_output, count):\n",
    "        count_token = self.count_embeding(count).unsqueeze(dim=1)\n",
    "\n",
    "        batch_size = bert_output.shape[0]\n",
    "\n",
    "        Q = self.proj_Q(self.layer_norm1(count_token)).view(batch_size, -1, self.num_head, int(self.hidden_size / self.num_head)).transpose(1, 2)\n",
    "        K = self.proj_K(self.layer_norm2(bert_output)).view(batch_size, -1, self.num_head, int(self.hidden_size / self.num_head)).transpose(1, 2)\n",
    "        V = self.proj_V(self.layer_norm2(bert_output)).view(batch_size, -1, self.num_head, int(self.hidden_size / self.num_head)).transpose(1, 2)\n",
    "\n",
    "        temp_output = self.linear_drop(self.linear(torch.matmul(self.attention_drop(F.softmax(torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.hidden_size / self.num_head), dim=-1)), V).transpose(1, 2).contiguous().view(batch_size, -1, self.hidden_size))).squeeze(dim=1) + bert_output[:, 0, :]\n",
    "\n",
    "        return self.fnn2(self.fnn_dropout(nn.ReLU()(self.fnn1(self.layer_norm3(temp_output))))) + temp_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertWithMLP_CrossAttention(nn.Module):\n",
    "    def __init__(self, bert, hidden_size=768, mlp_hidden_size1=1024, mlp_hidden_size2 =256, num_classes=10, num_count=332):\n",
    "        super(BertWithMLP_CrossAttention, self).__init__()\n",
    "        self.bert = bert\n",
    "        self.cross_attention = CrossAttention(num_count, 768, 768, 768, 12)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(hidden_size, mlp_hidden_size2),\n",
    "            nn.ReLU(),\n",
    "            # nn.Dropout(0.2),\n",
    "            # nn.Linear(mlp_hidden_size1, mlp_hidden_size2),\n",
    "            # nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(mlp_hidden_size2, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, count):\n",
    "\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        cls = self.cross_attention(outputs.last_hidden_state, count)\n",
    "\n",
    "        \n",
    "        logits = self.mlp(cls)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, optimizer, scheduler, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    # total_error = 0.0\n",
    "    \n",
    "    progress_bar = tqdm(dataloader, desc=\"Training\", leave=False)\n",
    "    for batch in progress_bar:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['rating'].to(device)\n",
    "        counts = batch['usefulcount'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, count=counts)\n",
    "\n",
    "        # loss = criterion(outputs, labels.float())\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        # preds = torch.round(outputs)\n",
    "        preds = torch.argmax(outputs, dim=-1)\n",
    "        correct_predictions += torch.sum(preds == labels)\n",
    "        # total_error += torch.sum(torch.abs(labels - outputs))\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # 更新进度条显示\n",
    "        progress_bar.set_postfix({\n",
    "            'loss': loss.item(),\n",
    "            'acc': torch.sum(preds == labels).item()/len(labels),\n",
    "            # 'error': torch.mean(torch.abs(labels - outputs)).item()\n",
    "        })\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = correct_predictions.double() / len(dataloader.dataset)\n",
    "    # error = total_error.item() / len(dataloader.dataset)\n",
    "    # return avg_loss, accuracy, error\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "\n",
    "def eval_model(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    # total_error = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        progress_bar = tqdm(dataloader, desc=\"Evaluating\", leave=False)\n",
    "        for batch in progress_bar:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['rating'].to(device)\n",
    "            counts = batch['usefulcount'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, count=counts)\n",
    "            loss = criterion(outputs, labels)\n",
    "            # loss = criterion(outputs, labels.float())\n",
    "            \n",
    "            preds = torch.argmax(outputs, dim=-1)\n",
    "            correct_predictions += torch.sum(preds == labels)\n",
    "            # total_error += torch.sum(torch.abs(labels - outputs))\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            progress_bar.set_postfix({\n",
    "                'loss': loss.item(),\n",
    "                'acc': torch.sum(preds == labels).item()/len(labels),\n",
    "                # 'error': torch.mean(torch.abs(labels - outputs)).item()\n",
    "            })\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = correct_predictions.double() / len(dataloader.dataset)\n",
    "    # error = total_error.item() / len(dataloader.dataset)\n",
    "    # return avg_loss, accuracy, error\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "# 4. 主训练循环\n",
    "def train_and_evaluate(\n",
    "    model, \n",
    "    train_loader, \n",
    "    val_loader, \n",
    "    optimizer, \n",
    "    scheduler,\n",
    "    criterion, \n",
    "    device, \n",
    "    epochs, \n",
    "    model_save_path,\n",
    "    eval_every=1  # 每多少轮评估一次\n",
    "):\n",
    "    # best_val_error = 0.0\n",
    "    best_val_acc = 0.0\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'train_acc': [],\n",
    "        # 'train_error': [],\n",
    "        'val_loss': [],\n",
    "        'val_acc': [],\n",
    "        # 'val_error': []\n",
    "    }\n",
    "    \n",
    "    for epoch in range(1, epochs+1):\n",
    "        print(f\"\\nEpoch {epoch}/{epochs}\")\n",
    "        \n",
    "        # 训练阶段\n",
    "        # train_loss, train_acc, train_error = train_epoch(\n",
    "        #     model, train_loader, optimizer, criterion, device)\n",
    "        train_loss, train_acc = train_epoch(\n",
    "            model, train_loader, optimizer, scheduler, criterion, device)\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc.item())\n",
    "        # history['train_error'].append(train_error)\n",
    "        \n",
    "        # print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | Train Error: {train_error:.4f}\")\n",
    "        print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
    "        \n",
    "        # 验证阶段\n",
    "        if epoch % eval_every == 0 and val_loader is not None:\n",
    "            # val_loss, val_acc, val_error = eval_model(\n",
    "            #     model, val_loader, criterion, device)\n",
    "            val_loss, val_acc = eval_model(\n",
    "                model, val_loader, criterion, device)\n",
    "            history['val_loss'].append(val_loss)\n",
    "            history['val_acc'].append(val_acc.item())\n",
    "            # history['val_error'].append(val_error)\n",
    "            \n",
    "            # print(f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f} | Val Error: {val_error:.4f}\")\n",
    "            print(f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "            \n",
    "            # if val_error > best_val_error:\n",
    "            #     best_val_error = val_error\n",
    "            #     torch.save(model.state_dict(), model_save_path)\n",
    "            #     print(f\"New best model saved to {model_save_path} with val_acc: {val_acc:.4f} | val_error: {val_error:.4f}\")\n",
    "\n",
    "            #     continue\n",
    "\n",
    "            # 保存最佳模型\n",
    "            if val_acc > best_val_acc:\n",
    "                best_val_acc = val_acc\n",
    "                torch.save(model.state_dict(), model_save_path)\n",
    "                # print(f\"New best model saved to {model_save_path} with val_acc: {val_acc:.4f} | val_error: {val_error:.4f}\")\n",
    "                print(f\"New best model saved to {model_save_path} with val_acc: {val_acc:.4f}\")\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.7350 | Train Acc: 0.3798\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 1.5470 | Val Acc: 0.4299\n",
      "New best model saved to ./rating_cross_attention_best_model.pth with val_acc: 0.4299\n",
      "\n",
      "Epoch 2/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.5017 | Train Acc: 0.4443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 1.4982 | Val Acc: 0.4411\n",
      "New best model saved to ./rating_cross_attention_best_model.pth with val_acc: 0.4411\n",
      "\n",
      "Epoch 3/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.3165 | Train Acc: 0.4997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 1.4471 | Val Acc: 0.4761\n",
      "New best model saved to ./rating_cross_attention_best_model.pth with val_acc: 0.4761\n",
      "\n",
      "Epoch 4/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                  \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 77\u001b[0m\n\u001b[0;32m     74\u001b[0m     \u001b[38;5;66;03m# print(f\"Best validation error: {max(history['val_error']):.4f}\")\u001b[39;00m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 77\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[25], line 59\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     56\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(model_save_path), exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     58\u001b[0m \u001b[38;5;66;03m# 训练和验证\u001b[39;00m\n\u001b[1;32m---> 59\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_and_evaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     62\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     63\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     64\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_save_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_save_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_every\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# 每轮都验证\u001b[39;49;00m\n\u001b[0;32m     70\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTraining complete!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest validation accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mmax\u001b[39m(history[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_acc\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[24], line 109\u001b[0m, in \u001b[0;36mtrain_and_evaluate\u001b[1;34m(model, train_loader, val_loader, optimizer, scheduler, criterion, device, epochs, model_save_path, eval_every)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    106\u001b[0m \u001b[38;5;66;03m# 训练阶段\u001b[39;00m\n\u001b[0;32m    107\u001b[0m \u001b[38;5;66;03m# train_loss, train_acc, train_error = train_epoch(\u001b[39;00m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;66;03m#     model, train_loader, optimizer, criterion, device)\u001b[39;00m\n\u001b[1;32m--> 109\u001b[0m train_loss, train_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    110\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    111\u001b[0m history[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_loss\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(train_loss)\n\u001b[0;32m    112\u001b[0m history[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_acc\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(train_acc\u001b[38;5;241m.\u001b[39mitem())\n",
      "Cell \u001b[1;32mIn[24], line 28\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[1;34m(model, dataloader, optimizer, scheduler, criterion, device)\u001b[0m\n\u001b[0;32m     26\u001b[0m correct_predictions \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(preds \u001b[38;5;241m==\u001b[39m labels)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# total_error += torch.sum(torch.abs(labels - outputs))\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# 更新进度条显示\u001b[39;00m\n\u001b[0;32m     31\u001b[0m progress_bar\u001b[38;5;241m.\u001b[39mset_postfix({\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m: loss\u001b[38;5;241m.\u001b[39mitem(),\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124macc\u001b[39m\u001b[38;5;124m'\u001b[39m: torch\u001b[38;5;241m.\u001b[39msum(preds \u001b[38;5;241m==\u001b[39m labels)\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(labels),\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;66;03m# 'error': torch.mean(torch.abs(labels - outputs)).item()\u001b[39;00m\n\u001b[0;32m     35\u001b[0m })\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # 初始化\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    BERT = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "    model = BertWithMLP_CrossAttention(BERT, hidden_size=768, mlp_hidden_size1=1024, mlp_hidden_size2=256, num_classes=10, num_count=2000)\n",
    "    model.to(device)\n",
    "\n",
    "    # 参数分组\n",
    "    no_decay = ['bias', 'LayerNorm.weight']\n",
    "    bert_params = []\n",
    "    mlp_params = []\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'mlp' in name or 'cross' in name:  # MLP层参数\n",
    "            mlp_params.append((name, param))\n",
    "        else:  # BERT参数\n",
    "            bert_params.append((name, param))\n",
    "\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in bert_params if not any(nd in n for nd in no_decay)],\n",
    "        'weight_decay': 0.01,\n",
    "        'lr': 2e-5},  # BERT主体较小学习率\n",
    "        \n",
    "        {'params': [p for n, p in bert_params if any(nd in n for nd in no_decay)],\n",
    "        'weight_decay': 0.0,\n",
    "        'lr': 2e-5},\n",
    "        \n",
    "        {'params': [p for n, p in mlp_params if not any(nd in n for nd in no_decay)],\n",
    "        'weight_decay': 0.01,\n",
    "        'lr': 1e-4},  # MLP层较大学习率\n",
    "        \n",
    "        {'params': [p for n, p in mlp_params if any(nd in n for nd in no_decay)],\n",
    "        'weight_decay': 0.0,\n",
    "        'lr': 1e-4}\n",
    "    ]\n",
    "\n",
    "    optimizer = AdamW(optimizer_grouped_parameters)\n",
    "\n",
    "    epochs = 15\n",
    "\n",
    "    total_steps = len(train_loader) * epochs\n",
    "    warmup_steps = int(0.1 * total_steps)  # 10%的warmup\n",
    "\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=warmup_steps,\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    model_save_path = \"./rating_cross_attention_best_model.pth\"\n",
    "    \n",
    "    # 创建保存目录\n",
    "    os.makedirs(os.path.dirname(model_save_path), exist_ok=True)\n",
    "    \n",
    "    # 训练和验证\n",
    "    history = train_and_evaluate(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=test_loader,\n",
    "        optimizer=optimizer,\n",
    "        scheduler=scheduler,\n",
    "        criterion=criterion,\n",
    "        device=device,\n",
    "        epochs=epochs,\n",
    "        model_save_path=model_save_path,\n",
    "        eval_every=1  # 每轮都验证\n",
    "    )\n",
    "    \n",
    "    print(\"\\nTraining complete!\")\n",
    "    print(f\"Best validation accuracy: {max(history['val_acc']):.4f}\")\n",
    "    # print(f\"Best validation error: {max(history['val_error']):.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
